<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>FER_Model_Training</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">tensorflow_version</span> 2.x
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>TensorFlow 2.x selected.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>2.0.0
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.keras</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="k">import</span> <span class="n">CSVLogger</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">EarlyStopping</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="k">import</span> <span class="n">ReduceLROnPlateau</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.image</span> <span class="k">import</span> <span class="n">ImageDataGenerator</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Convolution2D</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Conv2D</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> <span class="n">AveragePooling2D</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> <span class="n">GlobalAveragePooling2D</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="k">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> <span class="n">MaxPooling2D</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> <span class="n">SeparableConv2D</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="k">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.regularizers</span> <span class="k">import</span> <span class="n">l2</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.image</span> <span class="k">import</span> <span class="n">img_to_array</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="k">import</span> <span class="n">load_model</span>
<span class="kn">import</span> <span class="nn">imutils</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">preprocessing</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">skimage.util</span> <span class="k">import</span> <span class="n">random_noise</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>Import the faces and emotion labels appropriately</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dataset_path</span> <span class="o">=</span> <span class="s1">&#39;fer2013.csv&#39;</span>
<span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span><span class="mi">48</span><span class="p">)</span>
 
<span class="k">def</span> <span class="nf">load_fer2013</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">)</span>
    <span class="n">pixels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;pixels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span>
    <span class="n">faces</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pixel_sequence</span> <span class="ow">in</span> <span class="n">pixels</span><span class="p">:</span>
        <span class="n">face</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">pixel</span><span class="p">)</span> <span class="k">for</span> <span class="n">pixel</span> <span class="ow">in</span> <span class="n">pixel_sequence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)]</span>
        <span class="n">face</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">face</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>
        <span class="n">face</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">face</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">),</span><span class="n">image_size</span><span class="p">)</span>
        <span class="n">faces</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">face</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">))</span>
    <span class="n">faces</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">faces</span><span class="p">)</span>
    <span class="n">faces</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">faces</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">emotions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;emotion&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span>
    <span class="k">return</span> <span class="n">faces</span><span class="p">,</span> <span class="n">emotions</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Seven Categories:</p>
<p>(0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">faces</span><span class="p">,</span> <span class="n">emotions</span> <span class="o">=</span> <span class="n">load_fer2013</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">faces</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[11]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(48, 48, 1)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># openCV loads an image in BGR format - convert it into RGB format</span>
<span class="c1"># to display its true colors</span>
<span class="k">def</span> <span class="nf">convertToRGB</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">faces</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[13]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.image.AxesImage at 0x7fc15e7d8a58&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">faces</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sp_noise</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">prob</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Add salt and pepper noise to image</span>
<span class="sd">    prob: Probability of the noise</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">thres</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">prob</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">rdn</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">rdn</span> <span class="o">&lt;</span> <span class="n">prob</span><span class="p">:</span>
                <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">elif</span> <span class="n">rdn</span> <span class="o">&gt;</span> <span class="n">thres</span><span class="p">:</span>
                <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">image</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">noise_img</span> <span class="o">=</span> <span class="n">sp_noise</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mf">0.02</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">noise_img</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[18]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.image.AxesImage at 0x7fc15e5154a8&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">emotions</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0, 0, 1, 0, 0, 0, 0], dtype=uint8)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">saveImg</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="n">img</span><span class="p">):</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#saveImg(&#39;fear_48.jpg&#39;,faces[2])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">emotions</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0, 0, 0, 0, 1, 0, 0], dtype=uint8)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">GCN_HE</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">GCN</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">Hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  
  <span class="k">if</span> <span class="n">GCN</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">Hist</span><span class="p">:</span>  <span class="c1">#only GCN</span>
    <span class="n">img_pro</span>  <span class="o">=</span> <span class="n">global_contrast_normalization</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.000000001</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">GCN</span> <span class="ow">and</span> <span class="n">Hist</span><span class="p">:</span>
    <span class="n">img_pro</span>  <span class="o">=</span> <span class="n">global_contrast_normalization</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.000000001</span><span class="p">)</span>
    <span class="n">img_pro</span> <span class="o">=</span> <span class="n">histEqual</span><span class="p">(</span><span class="n">img_pro</span><span class="p">,</span><span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">Hist</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">GCN</span><span class="p">:</span>
    <span class="n">img_pro</span> <span class="o">=</span> <span class="n">histEqual</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  
  <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">img_pro</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">res</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">v2</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    scale the image to [0,1] for CNN</span>
<span class="sd">    (v2 scale the image to [-1,1] better range for CNN)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">/</span><span class="mi">255</span>
    <span class="k">if</span> <span class="n">v2</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="mf">0.5</span>
        <span class="n">x</span> <span class="o">*=</span> <span class="mf">2.0</span>
    <span class="k">return</span> <span class="n">x</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">global_contrast_normalization</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">lmda</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
    <span class="c1">#X = numpy.array(Image.open(filename))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">img</span>
    <span class="c1"># replacement for the loop</span>
    <span class="n">X_average</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X_average</span>

    <span class="c1"># `su` is here the mean, instead of the sum</span>
    <span class="n">contrast</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">lmda</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="n">X</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">contrast</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    
    <span class="c1"># Scale the X to 0,255 and round to integer</span>
    <span class="n">min_max_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">))</span>
    <span class="n">GCN_scaled</span> <span class="o">=</span> <span class="n">min_max_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">GCN_scaled</span> <span class="o">=</span> <span class="n">GCN_scaled</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">res</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">GCN_scaled</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">res</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">histEqual</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">img</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">img_ravel</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Original Hist:&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">img_ravel</span><span class="p">,</span><span class="mi">255</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="c1"># apply histogram equalization</span>
    <span class="n">img_hm</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">equalizeHist</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    
    <span class="n">img_hm_ravel</span> <span class="o">=</span> <span class="n">img_hm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Hist Equalized:&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">img_hm_ravel</span><span class="p">,</span><span class="mi">255</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_hm</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">img_hm</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">processFaces</span><span class="p">(</span><span class="n">faces</span><span class="p">):</span>
  <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">face</span> <span class="ow">in</span> <span class="n">faces</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">sp_noise</span><span class="p">(</span><span class="n">face</span><span class="p">,</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

  <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">results</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># If any image Processing is conducted</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">results</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">faces</span> <span class="o">=</span> <span class="n">preprocess_image</span><span class="p">(</span><span class="n">faces</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">xtrain</span><span class="p">,</span> <span class="n">xtest</span><span class="p">,</span><span class="n">ytrain</span><span class="p">,</span><span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">faces</span><span class="p">,</span> <span class="n">emotions</span><span class="p">,</span><span class="n">stratify</span><span class="o">=</span><span class="n">emotions</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">xtrain</span> <span class="o">=</span> <span class="n">processFaces</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">xtrain</span> <span class="o">=</span> <span class="n">preprocess_image</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">xtest</span> <span class="o">=</span> <span class="n">preprocess_image</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># get a list of class labels</span>
<span class="n">class_label</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>
<span class="c1"># initiate a list to store the images for each class</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># for each class</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">class_label</span><span class="p">:</span>
    <span class="c1"># get the index of the first image of class c</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)):</span>
        <span class="n">c_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ytrain</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="n">c_label</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xtrain</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">break</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">class_label_name</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Angry&#39;</span><span class="p">,</span> <span class="s1">&#39;Disgust&#39;</span><span class="p">,</span> <span class="s1">&#39;Fear&#39;</span><span class="p">,</span> <span class="s1">&#39;Happy&#39;</span><span class="p">,</span> <span class="s1">&#39;Sad&#39;</span><span class="p">,</span> <span class="s1">&#39;Surprise&#39;</span><span class="p">,</span> <span class="s1">&#39;Neutral&#39;</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">class_label_name</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
            
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAj8AAABgCAYAAADy6xM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cls_dist</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">cls_dist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cls_dist</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">emotions</span><span class="p">:</span>
    <span class="n">cls_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cls_dist</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{0: 4953, 1: 547, 2: 5121, 3: 8989, 4: 6077, 5: 4002, 6: 6198}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Raw Data</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">110</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">patience</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">base_path</span> <span class="o">=</span> <span class="s1">&#39;models/&#39;</span>
<span class="n">l2_regularization</span><span class="o">=</span><span class="mf">0.01</span>
 
<span class="c1"># data generator</span>
<span class="n">data_generator</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
                        <span class="n">featurewise_center</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">featurewise_std_normalization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">rotation_range</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">zoom_range</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
 
<span class="c1"># model parameters</span>
<span class="n">regularization</span> <span class="o">=</span> <span class="n">l2</span><span class="p">(</span><span class="n">l2_regularization</span><span class="p">)</span>
 
<span class="c1"># base</span>
<span class="n">img_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">img_input</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
 
<span class="c1"># module 1</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
 
<span class="c1"># module 2</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
 
<span class="c1"># module 3</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
 
<span class="c1"># module 4</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;predictions&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
 
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
 
<span class="c1"># callbacks</span>
<span class="n">log_file_path</span> <span class="o">=</span> <span class="n">base_path</span> <span class="o">+</span> <span class="s1">&#39;_emotion_training.log&#39;</span>
<span class="n">csv_logger</span> <span class="o">=</span> <span class="n">CSVLogger</span><span class="p">(</span><span class="n">log_file_path</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="n">patience</span><span class="p">)</span>
<span class="n">reduce_lr</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">patience</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">trained_models_path</span> <span class="o">=</span> <span class="n">base_path</span> <span class="o">+</span> <span class="s1">&#39;_mini_XCEPTION&#39;</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="n">trained_models_path</span> <span class="o">+</span> <span class="s1">&#39;.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_acc:.2f}</span><span class="s1">.hdf5&#39;</span>
<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_checkpoint</span><span class="p">,</span> <span class="n">csv_logger</span><span class="p">,</span> <span class="n">early_stop</span><span class="p">,</span> <span class="n">reduce_lr</span><span class="p">]</span>
 
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">data_generator</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span><span class="n">batch_size</span><span class="p">),</span>
                        <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: &#34;model_9&#34;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 48, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 46, 46, 8)    72          input_9[0][0]                    
__________________________________________________________________________________________________
batch_normalization_113 (BatchN (None, 46, 46, 8)    32          conv2d_57[0][0]                  
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 46, 46, 8)    0           batch_normalization_113[0][0]    
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 44, 44, 8)    576         activation_49[0][0]              
__________________________________________________________________________________________________
batch_normalization_114 (BatchN (None, 44, 44, 8)    32          conv2d_58[0][0]                  
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 44, 44, 8)    0           batch_normalization_114[0][0]    
__________________________________________________________________________________________________
separable_conv2d_65 (SeparableC (None, 44, 44, 16)   200         activation_50[0][0]              
__________________________________________________________________________________________________
batch_normalization_116 (BatchN (None, 44, 44, 16)   64          separable_conv2d_65[0][0]        
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 44, 44, 16)   0           batch_normalization_116[0][0]    
__________________________________________________________________________________________________
separable_conv2d_66 (SeparableC (None, 44, 44, 16)   400         activation_51[0][0]              
__________________________________________________________________________________________________
batch_normalization_117 (BatchN (None, 44, 44, 16)   64          separable_conv2d_66[0][0]        
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 22, 22, 16)   128         activation_50[0][0]              
__________________________________________________________________________________________________
max_pooling2d_33 (MaxPooling2D) (None, 22, 22, 16)   0           batch_normalization_117[0][0]    
__________________________________________________________________________________________________
batch_normalization_115 (BatchN (None, 22, 22, 16)   64          conv2d_59[0][0]                  
__________________________________________________________________________________________________
add_33 (Add)                    (None, 22, 22, 16)   0           max_pooling2d_33[0][0]           
                                                                 batch_normalization_115[0][0]    
__________________________________________________________________________________________________
separable_conv2d_67 (SeparableC (None, 22, 22, 32)   656         add_33[0][0]                     
__________________________________________________________________________________________________
batch_normalization_119 (BatchN (None, 22, 22, 32)   128         separable_conv2d_67[0][0]        
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 22, 22, 32)   0           batch_normalization_119[0][0]    
__________________________________________________________________________________________________
separable_conv2d_68 (SeparableC (None, 22, 22, 32)   1312        activation_52[0][0]              
__________________________________________________________________________________________________
batch_normalization_120 (BatchN (None, 22, 22, 32)   128         separable_conv2d_68[0][0]        
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 11, 11, 32)   512         add_33[0][0]                     
__________________________________________________________________________________________________
max_pooling2d_34 (MaxPooling2D) (None, 11, 11, 32)   0           batch_normalization_120[0][0]    
__________________________________________________________________________________________________
batch_normalization_118 (BatchN (None, 11, 11, 32)   128         conv2d_60[0][0]                  
__________________________________________________________________________________________________
add_34 (Add)                    (None, 11, 11, 32)   0           max_pooling2d_34[0][0]           
                                                                 batch_normalization_118[0][0]    
__________________________________________________________________________________________________
separable_conv2d_69 (SeparableC (None, 11, 11, 64)   2336        add_34[0][0]                     
__________________________________________________________________________________________________
batch_normalization_122 (BatchN (None, 11, 11, 64)   256         separable_conv2d_69[0][0]        
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 11, 11, 64)   0           batch_normalization_122[0][0]    
__________________________________________________________________________________________________
separable_conv2d_70 (SeparableC (None, 11, 11, 64)   4672        activation_53[0][0]              
__________________________________________________________________________________________________
batch_normalization_123 (BatchN (None, 11, 11, 64)   256         separable_conv2d_70[0][0]        
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 6, 6, 64)     2048        add_34[0][0]                     
__________________________________________________________________________________________________
max_pooling2d_35 (MaxPooling2D) (None, 6, 6, 64)     0           batch_normalization_123[0][0]    
__________________________________________________________________________________________________
batch_normalization_121 (BatchN (None, 6, 6, 64)     256         conv2d_61[0][0]                  
__________________________________________________________________________________________________
add_35 (Add)                    (None, 6, 6, 64)     0           max_pooling2d_35[0][0]           
                                                                 batch_normalization_121[0][0]    
__________________________________________________________________________________________________
separable_conv2d_71 (SeparableC (None, 6, 6, 128)    8768        add_35[0][0]                     
__________________________________________________________________________________________________
batch_normalization_125 (BatchN (None, 6, 6, 128)    512         separable_conv2d_71[0][0]        
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 6, 6, 128)    0           batch_normalization_125[0][0]    
__________________________________________________________________________________________________
separable_conv2d_72 (SeparableC (None, 6, 6, 128)    17536       activation_54[0][0]              
__________________________________________________________________________________________________
batch_normalization_126 (BatchN (None, 6, 6, 128)    512         separable_conv2d_72[0][0]        
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 3, 3, 128)    8192        add_35[0][0]                     
__________________________________________________________________________________________________
max_pooling2d_36 (MaxPooling2D) (None, 3, 3, 128)    0           batch_normalization_126[0][0]    
__________________________________________________________________________________________________
batch_normalization_124 (BatchN (None, 3, 3, 128)    512         conv2d_62[0][0]                  
__________________________________________________________________________________________________
add_36 (Add)                    (None, 3, 3, 128)    0           max_pooling2d_36[0][0]           
                                                                 batch_normalization_124[0][0]    
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 3, 3, 7)      8071        add_36[0][0]                     
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 7)            0           conv2d_63[0][0]                  
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
Epoch 1/110
898/897 [==============================] - 58s 65ms/step - loss: 1.7746 - acc: 0.3310 - val_loss: 1.5998 - val_acc: 0.4168

Epoch 00001: val_loss improved from inf to 1.59976, saving model to models/_mini_XCEPTION.01-0.42.hdf5
Epoch 2/110
898/897 [==============================] - 49s 54ms/step - loss: 1.5126 - acc: 0.4338 - val_loss: 1.4845 - val_acc: 0.4558

Epoch 00002: val_loss improved from 1.59976 to 1.48447, saving model to models/_mini_XCEPTION.02-0.46.hdf5
Epoch 3/110
898/897 [==============================] - 48s 54ms/step - loss: 1.3890 - acc: 0.4815 - val_loss: 1.4225 - val_acc: 0.4654

Epoch 00003: val_loss improved from 1.48447 to 1.42251, saving model to models/_mini_XCEPTION.03-0.47.hdf5
Epoch 4/110
898/897 [==============================] - 49s 54ms/step - loss: 1.3228 - acc: 0.5068 - val_loss: 1.2862 - val_acc: 0.5298

Epoch 00004: val_loss improved from 1.42251 to 1.28621, saving model to models/_mini_XCEPTION.04-0.53.hdf5
Epoch 5/110
898/897 [==============================] - 49s 54ms/step - loss: 1.2743 - acc: 0.5214 - val_loss: 1.2398 - val_acc: 0.5368

Epoch 00005: val_loss improved from 1.28621 to 1.23981, saving model to models/_mini_XCEPTION.05-0.54.hdf5
Epoch 6/110
898/897 [==============================] - 49s 55ms/step - loss: 1.2391 - acc: 0.5359 - val_loss: 1.1977 - val_acc: 0.5468

Epoch 00006: val_loss improved from 1.23981 to 1.19774, saving model to models/_mini_XCEPTION.06-0.55.hdf5
Epoch 7/110
898/897 [==============================] - 49s 54ms/step - loss: 1.2059 - acc: 0.5479 - val_loss: 1.2542 - val_acc: 0.5368

Epoch 00007: val_loss did not improve from 1.19774
Epoch 8/110
898/897 [==============================] - 48s 54ms/step - loss: 1.1898 - acc: 0.5524 - val_loss: 1.2439 - val_acc: 0.5368

Epoch 00008: val_loss did not improve from 1.19774
Epoch 9/110
898/897 [==============================] - 48s 54ms/step - loss: 1.1734 - acc: 0.5584 - val_loss: 1.1716 - val_acc: 0.5589

Epoch 00009: val_loss improved from 1.19774 to 1.17157, saving model to models/_mini_XCEPTION.09-0.56.hdf5
Epoch 10/110
898/897 [==============================] - 48s 53ms/step - loss: 1.1538 - acc: 0.5707 - val_loss: 1.2016 - val_acc: 0.5561

Epoch 00010: val_loss did not improve from 1.17157
Epoch 11/110
898/897 [==============================] - 49s 55ms/step - loss: 1.1407 - acc: 0.5751 - val_loss: 1.1814 - val_acc: 0.5607

Epoch 00011: val_loss did not improve from 1.17157
Epoch 12/110
898/897 [==============================] - 48s 54ms/step - loss: 1.1245 - acc: 0.5798 - val_loss: 1.1388 - val_acc: 0.5819

Epoch 00012: val_loss improved from 1.17157 to 1.13879, saving model to models/_mini_XCEPTION.12-0.58.hdf5
Epoch 13/110
898/897 [==============================] - 48s 54ms/step - loss: 1.1121 - acc: 0.5825 - val_loss: 1.2533 - val_acc: 0.5403

Epoch 00013: val_loss did not improve from 1.13879
Epoch 14/110
898/897 [==============================] - 48s 54ms/step - loss: 1.1069 - acc: 0.5874 - val_loss: 1.1113 - val_acc: 0.5943

Epoch 00014: val_loss improved from 1.13879 to 1.11133, saving model to models/_mini_XCEPTION.14-0.59.hdf5
Epoch 15/110
898/897 [==============================] - 49s 54ms/step - loss: 1.0976 - acc: 0.5883 - val_loss: 1.1740 - val_acc: 0.5756

Epoch 00015: val_loss did not improve from 1.11133
Epoch 16/110
898/897 [==============================] - 48s 54ms/step - loss: 1.0861 - acc: 0.5927 - val_loss: 1.1244 - val_acc: 0.5851

Epoch 00016: val_loss did not improve from 1.11133
Epoch 17/110
898/897 [==============================] - 49s 54ms/step - loss: 1.0775 - acc: 0.5995 - val_loss: 1.1250 - val_acc: 0.5823

Epoch 00017: val_loss did not improve from 1.11133
Epoch 18/110
898/897 [==============================] - 49s 54ms/step - loss: 1.0753 - acc: 0.5974 - val_loss: 1.1329 - val_acc: 0.5843

Epoch 00018: val_loss did not improve from 1.11133
Epoch 19/110
898/897 [==============================] - 48s 54ms/step - loss: 1.0639 - acc: 0.6029 - val_loss: 1.0901 - val_acc: 0.5964

Epoch 00019: val_loss improved from 1.11133 to 1.09012, saving model to models/_mini_XCEPTION.19-0.60.hdf5
Epoch 20/110
898/897 [==============================] - 49s 54ms/step - loss: 1.0525 - acc: 0.6082 - val_loss: 1.0928 - val_acc: 0.6018

Epoch 00020: val_loss did not improve from 1.09012
Epoch 21/110
898/897 [==============================] - 48s 54ms/step - loss: 1.0491 - acc: 0.6103 - val_loss: 1.1105 - val_acc: 0.5897

Epoch 00021: val_loss did not improve from 1.09012
Epoch 22/110
898/897 [==============================] - 49s 54ms/step - loss: 1.0485 - acc: 0.6104 - val_loss: 1.0775 - val_acc: 0.6081

Epoch 00022: val_loss improved from 1.09012 to 1.07753, saving model to models/_mini_XCEPTION.22-0.61.hdf5
Epoch 23/110
898/897 [==============================] - 49s 54ms/step - loss: 1.0387 - acc: 0.6133 - val_loss: 1.0947 - val_acc: 0.5932

Epoch 00023: val_loss did not improve from 1.07753
Epoch 24/110
898/897 [==============================] - 48s 54ms/step - loss: 1.0307 - acc: 0.6151 - val_loss: 1.0955 - val_acc: 0.5953

Epoch 00024: val_loss did not improve from 1.07753
Epoch 25/110
898/897 [==============================] - 48s 54ms/step - loss: 1.0303 - acc: 0.6128 - val_loss: 1.0954 - val_acc: 0.5872

Epoch 00025: val_loss did not improve from 1.07753
Epoch 26/110
898/897 [==============================] - 51s 56ms/step - loss: 1.0218 - acc: 0.6176 - val_loss: 1.0817 - val_acc: 0.6002

Epoch 00026: val_loss did not improve from 1.07753
Epoch 27/110
898/897 [==============================] - 51s 56ms/step - loss: 1.0125 - acc: 0.6261 - val_loss: 1.0927 - val_acc: 0.5968

Epoch 00027: val_loss did not improve from 1.07753
Epoch 28/110
898/897 [==============================] - 49s 54ms/step - loss: 1.0162 - acc: 0.6199 - val_loss: 1.0430 - val_acc: 0.6130

Epoch 00028: val_loss improved from 1.07753 to 1.04299, saving model to models/_mini_XCEPTION.28-0.61.hdf5
Epoch 29/110
898/897 [==============================] - 49s 55ms/step - loss: 1.0071 - acc: 0.6239 - val_loss: 1.0621 - val_acc: 0.6121

Epoch 00029: val_loss did not improve from 1.04299
Epoch 30/110
898/897 [==============================] - 48s 53ms/step - loss: 1.0078 - acc: 0.6236 - val_loss: 1.0570 - val_acc: 0.6109

Epoch 00030: val_loss did not improve from 1.04299
Epoch 31/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9966 - acc: 0.6306 - val_loss: 1.1377 - val_acc: 0.5900

Epoch 00031: val_loss did not improve from 1.04299
Epoch 32/110
898/897 [==============================] - 49s 55ms/step - loss: 0.9965 - acc: 0.6253 - val_loss: 1.0465 - val_acc: 0.6133

Epoch 00032: val_loss did not improve from 1.04299
Epoch 33/110
898/897 [==============================] - 49s 55ms/step - loss: 0.9935 - acc: 0.6286 - val_loss: 1.0458 - val_acc: 0.6124

Epoch 00033: val_loss did not improve from 1.04299
Epoch 34/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9919 - acc: 0.6319 - val_loss: 1.0391 - val_acc: 0.6176

Epoch 00034: val_loss improved from 1.04299 to 1.03910, saving model to models/_mini_XCEPTION.34-0.62.hdf5
Epoch 35/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9895 - acc: 0.6304 - val_loss: 1.0491 - val_acc: 0.6148

Epoch 00035: val_loss did not improve from 1.03910
Epoch 36/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9828 - acc: 0.6321 - val_loss: 1.0374 - val_acc: 0.6191

Epoch 00036: val_loss improved from 1.03910 to 1.03739, saving model to models/_mini_XCEPTION.36-0.62.hdf5
Epoch 37/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9778 - acc: 0.6364 - val_loss: 1.0883 - val_acc: 0.6092

Epoch 00037: val_loss did not improve from 1.03739
Epoch 38/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9782 - acc: 0.6353 - val_loss: 1.0419 - val_acc: 0.6245

Epoch 00038: val_loss did not improve from 1.03739
Epoch 39/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9732 - acc: 0.6359 - val_loss: 1.0315 - val_acc: 0.6155

Epoch 00039: val_loss improved from 1.03739 to 1.03150, saving model to models/_mini_XCEPTION.39-0.62.hdf5
Epoch 40/110
898/897 [==============================] - 49s 54ms/step - loss: 0.9739 - acc: 0.6383 - val_loss: 1.0531 - val_acc: 0.6149

Epoch 00040: val_loss did not improve from 1.03150
Epoch 41/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9691 - acc: 0.6418 - val_loss: 1.0255 - val_acc: 0.6181

Epoch 00041: val_loss improved from 1.03150 to 1.02554, saving model to models/_mini_XCEPTION.41-0.62.hdf5
Epoch 42/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9673 - acc: 0.6393 - val_loss: 1.0148 - val_acc: 0.6287

Epoch 00042: val_loss improved from 1.02554 to 1.01476, saving model to models/_mini_XCEPTION.42-0.63.hdf5
Epoch 43/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9635 - acc: 0.6418 - val_loss: 1.0440 - val_acc: 0.6155

Epoch 00043: val_loss did not improve from 1.01476
Epoch 44/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9603 - acc: 0.6411 - val_loss: 1.0747 - val_acc: 0.6102

Epoch 00044: val_loss did not improve from 1.01476
Epoch 45/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9542 - acc: 0.6425 - val_loss: 1.0765 - val_acc: 0.6041

Epoch 00045: val_loss did not improve from 1.01476
Epoch 46/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9529 - acc: 0.6441 - val_loss: 1.0578 - val_acc: 0.6095

Epoch 00046: val_loss did not improve from 1.01476
Epoch 47/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9533 - acc: 0.6446 - val_loss: 1.0208 - val_acc: 0.6258

Epoch 00047: val_loss did not improve from 1.01476
Epoch 48/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9492 - acc: 0.6470 - val_loss: 1.0877 - val_acc: 0.5957

Epoch 00048: val_loss did not improve from 1.01476
Epoch 49/110
898/897 [==============================] - 49s 55ms/step - loss: 0.9469 - acc: 0.6459 - val_loss: 1.0928 - val_acc: 0.5984

Epoch 00049: val_loss did not improve from 1.01476
Epoch 50/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9457 - acc: 0.6471 - val_loss: 1.0484 - val_acc: 0.6108

Epoch 00050: val_loss did not improve from 1.01476
Epoch 51/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9407 - acc: 0.6484 - val_loss: 1.0178 - val_acc: 0.6266

Epoch 00051: val_loss did not improve from 1.01476
Epoch 52/110
898/897 [==============================] - 48s 54ms/step - loss: 0.9386 - acc: 0.6526 - val_loss: 1.0344 - val_acc: 0.6206

Epoch 00052: val_loss did not improve from 1.01476
Epoch 53/110
898/897 [==============================] - 49s 54ms/step - loss: 0.9408 - acc: 0.6509 - val_loss: 1.0368 - val_acc: 0.6180

Epoch 00053: val_loss did not improve from 1.01476
Epoch 54/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9395 - acc: 0.6493 - val_loss: 1.0756 - val_acc: 0.6031

Epoch 00054: val_loss did not improve from 1.01476

Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 55/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8876 - acc: 0.6676 - val_loss: 0.9809 - val_acc: 0.6374

Epoch 00055: val_loss improved from 1.01476 to 0.98087, saving model to models/_mini_XCEPTION.55-0.64.hdf5
Epoch 56/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8752 - acc: 0.6755 - val_loss: 0.9769 - val_acc: 0.6417

Epoch 00056: val_loss improved from 0.98087 to 0.97693, saving model to models/_mini_XCEPTION.56-0.64.hdf5
Epoch 57/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8691 - acc: 0.6768 - val_loss: 0.9780 - val_acc: 0.6414

Epoch 00057: val_loss did not improve from 0.97693
Epoch 58/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8632 - acc: 0.6772 - val_loss: 0.9762 - val_acc: 0.6392

Epoch 00058: val_loss improved from 0.97693 to 0.97620, saving model to models/_mini_XCEPTION.58-0.64.hdf5
Epoch 59/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8586 - acc: 0.6799 - val_loss: 0.9779 - val_acc: 0.6414

Epoch 00059: val_loss did not improve from 0.97620
Epoch 60/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8587 - acc: 0.6807 - val_loss: 0.9764 - val_acc: 0.6445

Epoch 00060: val_loss did not improve from 0.97620
Epoch 61/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8524 - acc: 0.6835 - val_loss: 0.9743 - val_acc: 0.6446

Epoch 00061: val_loss improved from 0.97620 to 0.97428, saving model to models/_mini_XCEPTION.61-0.64.hdf5
Epoch 62/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8556 - acc: 0.6828 - val_loss: 0.9773 - val_acc: 0.6413

Epoch 00062: val_loss did not improve from 0.97428
Epoch 63/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8528 - acc: 0.6811 - val_loss: 0.9692 - val_acc: 0.6410

Epoch 00063: val_loss improved from 0.97428 to 0.96915, saving model to models/_mini_XCEPTION.63-0.64.hdf5
Epoch 64/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8532 - acc: 0.6830 - val_loss: 0.9729 - val_acc: 0.6441

Epoch 00064: val_loss did not improve from 0.96915
Epoch 65/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8488 - acc: 0.6842 - val_loss: 0.9768 - val_acc: 0.6449

Epoch 00065: val_loss did not improve from 0.96915
Epoch 66/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8461 - acc: 0.6862 - val_loss: 0.9741 - val_acc: 0.6457

Epoch 00066: val_loss did not improve from 0.96915
Epoch 67/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8461 - acc: 0.6829 - val_loss: 0.9761 - val_acc: 0.6404

Epoch 00067: val_loss did not improve from 0.96915
Epoch 68/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8470 - acc: 0.6855 - val_loss: 0.9722 - val_acc: 0.6460

Epoch 00068: val_loss did not improve from 0.96915
Epoch 69/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8451 - acc: 0.6839 - val_loss: 0.9709 - val_acc: 0.6484

Epoch 00069: val_loss did not improve from 0.96915
Epoch 70/110
898/897 [==============================] - 49s 55ms/step - loss: 0.8419 - acc: 0.6874 - val_loss: 0.9805 - val_acc: 0.6413

Epoch 00070: val_loss did not improve from 0.96915
Epoch 71/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8417 - acc: 0.6877 - val_loss: 0.9739 - val_acc: 0.6475

Epoch 00071: val_loss did not improve from 0.96915
Epoch 72/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8435 - acc: 0.6873 - val_loss: 0.9734 - val_acc: 0.6431

Epoch 00072: val_loss did not improve from 0.96915
Epoch 73/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8429 - acc: 0.6877 - val_loss: 0.9784 - val_acc: 0.6446

Epoch 00073: val_loss did not improve from 0.96915
Epoch 74/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8368 - acc: 0.6883 - val_loss: 0.9726 - val_acc: 0.6449

Epoch 00074: val_loss did not improve from 0.96915
Epoch 75/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8440 - acc: 0.6872 - val_loss: 0.9845 - val_acc: 0.6434

Epoch 00075: val_loss did not improve from 0.96915

Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 76/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8323 - acc: 0.6942 - val_loss: 0.9739 - val_acc: 0.6446

Epoch 00076: val_loss did not improve from 0.96915
Epoch 77/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8333 - acc: 0.6903 - val_loss: 0.9727 - val_acc: 0.6454

Epoch 00077: val_loss did not improve from 0.96915
Epoch 78/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8306 - acc: 0.6918 - val_loss: 0.9735 - val_acc: 0.6447

Epoch 00078: val_loss did not improve from 0.96915
Epoch 79/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8325 - acc: 0.6874 - val_loss: 0.9716 - val_acc: 0.6463

Epoch 00079: val_loss did not improve from 0.96915
Epoch 80/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8284 - acc: 0.6915 - val_loss: 0.9723 - val_acc: 0.6457

Epoch 00080: val_loss did not improve from 0.96915
Epoch 81/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8319 - acc: 0.6904 - val_loss: 0.9721 - val_acc: 0.6468

Epoch 00081: val_loss did not improve from 0.96915
Epoch 82/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8323 - acc: 0.6896 - val_loss: 0.9735 - val_acc: 0.6461

Epoch 00082: val_loss did not improve from 0.96915
Epoch 83/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8275 - acc: 0.6922 - val_loss: 0.9731 - val_acc: 0.6470

Epoch 00083: val_loss did not improve from 0.96915
Epoch 84/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8283 - acc: 0.6898 - val_loss: 0.9712 - val_acc: 0.6457

Epoch 00084: val_loss did not improve from 0.96915
Epoch 85/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8284 - acc: 0.6942 - val_loss: 0.9730 - val_acc: 0.6457

Epoch 00085: val_loss did not improve from 0.96915
Epoch 86/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8276 - acc: 0.6897 - val_loss: 0.9709 - val_acc: 0.6456

Epoch 00086: val_loss did not improve from 0.96915
Epoch 87/110
898/897 [==============================] - 49s 55ms/step - loss: 0.8273 - acc: 0.6924 - val_loss: 0.9726 - val_acc: 0.6457

Epoch 00087: val_loss did not improve from 0.96915

Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Epoch 88/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8263 - acc: 0.6922 - val_loss: 0.9727 - val_acc: 0.6466

Epoch 00088: val_loss did not improve from 0.96915
Epoch 89/110
898/897 [==============================] - 52s 58ms/step - loss: 0.8247 - acc: 0.6934 - val_loss: 0.9721 - val_acc: 0.6456

Epoch 00089: val_loss did not improve from 0.96915
Epoch 90/110
898/897 [==============================] - 49s 55ms/step - loss: 0.8315 - acc: 0.6896 - val_loss: 0.9710 - val_acc: 0.6467

Epoch 00090: val_loss did not improve from 0.96915
Epoch 91/110
898/897 [==============================] - 50s 55ms/step - loss: 0.8343 - acc: 0.6902 - val_loss: 0.9707 - val_acc: 0.6461

Epoch 00091: val_loss did not improve from 0.96915
Epoch 92/110
898/897 [==============================] - 50s 55ms/step - loss: 0.8228 - acc: 0.6945 - val_loss: 0.9713 - val_acc: 0.6470

Epoch 00092: val_loss did not improve from 0.96915
Epoch 93/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8301 - acc: 0.6938 - val_loss: 0.9718 - val_acc: 0.6454

Epoch 00093: val_loss did not improve from 0.96915
Epoch 94/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8292 - acc: 0.6895 - val_loss: 0.9706 - val_acc: 0.6467

Epoch 00094: val_loss did not improve from 0.96915
Epoch 95/110
898/897 [==============================] - 50s 56ms/step - loss: 0.8292 - acc: 0.6925 - val_loss: 0.9715 - val_acc: 0.6464

Epoch 00095: val_loss did not improve from 0.96915
Epoch 96/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8285 - acc: 0.6915 - val_loss: 0.9712 - val_acc: 0.6468

Epoch 00096: val_loss did not improve from 0.96915
Epoch 97/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8272 - acc: 0.6920 - val_loss: 0.9723 - val_acc: 0.6457

Epoch 00097: val_loss did not improve from 0.96915
Epoch 98/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8256 - acc: 0.6926 - val_loss: 0.9702 - val_acc: 0.6470

Epoch 00098: val_loss did not improve from 0.96915
Epoch 99/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8293 - acc: 0.6913 - val_loss: 0.9725 - val_acc: 0.6454

Epoch 00099: val_loss did not improve from 0.96915

Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
Epoch 100/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8231 - acc: 0.6931 - val_loss: 0.9723 - val_acc: 0.6452

Epoch 00100: val_loss did not improve from 0.96915
Epoch 101/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8307 - acc: 0.6905 - val_loss: 0.9718 - val_acc: 0.6459

Epoch 00101: val_loss did not improve from 0.96915
Epoch 102/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8264 - acc: 0.6923 - val_loss: 0.9726 - val_acc: 0.6454

Epoch 00102: val_loss did not improve from 0.96915
Epoch 103/110
898/897 [==============================] - 49s 54ms/step - loss: 0.8280 - acc: 0.6934 - val_loss: 0.9713 - val_acc: 0.6468

Epoch 00103: val_loss did not improve from 0.96915
Epoch 104/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8268 - acc: 0.6957 - val_loss: 0.9716 - val_acc: 0.6463

Epoch 00104: val_loss did not improve from 0.96915
Epoch 105/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8246 - acc: 0.6947 - val_loss: 0.9723 - val_acc: 0.6446

Epoch 00105: val_loss did not improve from 0.96915
Epoch 106/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8252 - acc: 0.6939 - val_loss: 0.9711 - val_acc: 0.6464

Epoch 00106: val_loss did not improve from 0.96915
Epoch 107/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8252 - acc: 0.6922 - val_loss: 0.9722 - val_acc: 0.6454

Epoch 00107: val_loss did not improve from 0.96915
Epoch 108/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8334 - acc: 0.6895 - val_loss: 0.9710 - val_acc: 0.6460

Epoch 00108: val_loss did not improve from 0.96915
Epoch 109/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8279 - acc: 0.6914 - val_loss: 0.9724 - val_acc: 0.6460

Epoch 00109: val_loss did not improve from 0.96915
Epoch 110/110
898/897 [==============================] - 48s 54ms/step - loss: 0.8272 - acc: 0.6924 - val_loss: 0.9716 - val_acc: 0.6456

Epoch 00110: val_loss did not improve from 0.96915
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x7f46cecc9e10&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">emotion_model_path</span> <span class="o">=</span> <span class="s1">&#39;_mini_XCEPTION.76-0.65.hdf5&#39;</span>
<span class="n">emotion_classifier</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">emotion_model_path</span><span class="p">,</span><span class="nb">compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gainClassification_report</span><span class="p">(</span><span class="n">emotion_classifier</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 618   16   57   26  114   22  138]
 [  18   72    6    1    6    4    2]
 [ 125    3  473   27  195   84  117]
 [  29    0   34 1575   39   29   92]
 [ 108    1   92   27  732   13  243]
 [  19    1   78   35   13  628   26]
 [  76    1   32   56  125   13  937]]
Classification Report
              precision    recall  f1-score   support

       angry       0.62      0.62      0.62       991
     disgust       0.77      0.66      0.71       109
        fear       0.61      0.46      0.53      1024
       happy       0.90      0.88      0.89      1798
         sad       0.60      0.60      0.60      1216
   surprised       0.79      0.79      0.79       800
     neutral       0.60      0.76      0.67      1240

    accuracy                           0.70      7178
   macro avg       0.70      0.68      0.69      7178
weighted avg       0.70      0.70      0.70      7178

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(array([[ 618,   16,   57,   26,  114,   22,  138],
        [  18,   72,    6,    1,    6,    4,    2],
        [ 125,    3,  473,   27,  195,   84,  117],
        [  29,    0,   34, 1575,   39,   29,   92],
        [ 108,    1,   92,   27,  732,   13,  243],
        [  19,    1,   78,   35,   13,  628,   26],
        [  76,    1,   32,   56,  125,   13,  937]]),
 &#39;              precision    recall  f1-score   support\n\n       angry       0.62      0.62      0.62       991\n     disgust       0.77      0.66      0.71       109\n        fear       0.61      0.46      0.53      1024\n       happy       0.90      0.88      0.89      1798\n         sad       0.60      0.60      0.60      1216\n   surprised       0.79      0.79      0.79       800\n     neutral       0.60      0.76      0.67      1240\n\n    accuracy                           0.70      7178\n   macro avg       0.70      0.68      0.69      7178\nweighted avg       0.70      0.70      0.70      7178\n&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[70]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gainClassification_report</span><span class="p">(</span><span class="n">emotion_classifier</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 615    9   54   35  118   22  138]
 [  14   66    7    0   13    2    7]
 [ 141    4  463   21  194   71  130]
 [  43    0   28 1553   37   42   95]
 [ 111    4   85   29  721    7  259]
 [  30    2   78   27   13  626   24]
 [  66    1   30   74  131   21  917]]
Classification Report
              precision    recall  f1-score   support

       angry       0.60      0.62      0.61       991
     disgust       0.77      0.61      0.68       109
        fear       0.62      0.45      0.52      1024
       happy       0.89      0.86      0.88      1798
         sad       0.59      0.59      0.59      1216
   surprised       0.79      0.78      0.79       800
     neutral       0.58      0.74      0.65      1240

    accuracy                           0.69      7178
   macro avg       0.69      0.67      0.67      7178
weighted avg       0.70      0.69      0.69      7178

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[70]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(array([[ 615,    9,   54,   35,  118,   22,  138],
        [  14,   66,    7,    0,   13,    2,    7],
        [ 141,    4,  463,   21,  194,   71,  130],
        [  43,    0,   28, 1553,   37,   42,   95],
        [ 111,    4,   85,   29,  721,    7,  259],
        [  30,    2,   78,   27,   13,  626,   24],
        [  66,    1,   30,   74,  131,   21,  917]]),
 &#39;              precision    recall  f1-score   support\n\n       angry       0.60      0.62      0.61       991\n     disgust       0.77      0.61      0.68       109\n        fear       0.62      0.45      0.52      1024\n       happy       0.89      0.86      0.88      1798\n         sad       0.59      0.59      0.59      1216\n   surprised       0.79      0.78      0.79       800\n     neutral       0.58      0.74      0.65      1240\n\n    accuracy                           0.69      7178\n   macro avg       0.69      0.67      0.67      7178\nweighted avg       0.70      0.69      0.69      7178\n&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">trainCNN</span><span class="p">():</span>
  <span class="c1"># parameters</span>
  <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
  <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
  <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">7</span>
  <span class="n">patience</span> <span class="o">=</span> <span class="mi">50</span>
  <span class="n">base_path</span> <span class="o">=</span> <span class="n">save_path</span>
  <span class="n">l2_regularization</span><span class="o">=</span><span class="mf">0.01</span>
  
  <span class="c1"># data generator</span>
  <span class="n">data_generator</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
                          <span class="n">featurewise_center</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">featurewise_std_normalization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">rotation_range</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                          <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                          <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                          <span class="n">zoom_range</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  
  <span class="c1"># model parameters</span>
  <span class="n">regularization</span> <span class="o">=</span> <span class="n">l2</span><span class="p">(</span><span class="n">l2_regularization</span><span class="p">)</span>
  
  <span class="c1"># base</span>
  <span class="n">img_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">img_input</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  
  <span class="c1"># module 1</span>
  <span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
  
  <span class="c1"># module 2</span>
  <span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
  
  <span class="c1"># module 3</span>
  <span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
  
  <span class="c1"># module 4</span>
  <span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;predictions&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  
  <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
  <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
  <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
  
  <span class="c1"># callbacks</span>
  <span class="n">log_file_path</span> <span class="o">=</span> <span class="n">base_path</span> <span class="o">+</span> <span class="s1">&#39;_emotion_training.log&#39;</span>
  <span class="n">csv_logger</span> <span class="o">=</span> <span class="n">CSVLogger</span><span class="p">(</span><span class="n">log_file_path</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="n">patience</span><span class="p">)</span>
  <span class="n">reduce_lr</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">patience</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">trained_models_path</span> <span class="o">=</span> <span class="n">base_path</span> <span class="o">+</span> <span class="s1">&#39;_mini_XCEPTION_Noise_Train&#39;</span>
  <span class="n">model_names</span> <span class="o">=</span> <span class="n">trained_models_path</span> <span class="o">+</span> <span class="s1">&#39;.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_accuracy:.2f}</span><span class="s1">.hdf5&#39;</span>
  <span class="n">model_checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_checkpoint</span><span class="p">,</span> <span class="n">csv_logger</span><span class="p">,</span> <span class="n">early_stop</span><span class="p">,</span> <span class="n">reduce_lr</span><span class="p">]</span>
  
  <span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">data_generator</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span><span class="n">batch_size</span><span class="p">),</span>
                          <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span>
                          <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="New-Model-Training-GCN+HE">New Model Training GCN+HE<a class="anchor-link" href="#New-Model-Training-GCN+HE">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">110</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">patience</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">base_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
<span class="n">l2_regularization</span><span class="o">=</span><span class="mf">0.01</span>
 
<span class="c1"># data generator</span>
<span class="n">data_generator</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
                        <span class="n">featurewise_center</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">featurewise_std_normalization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">rotation_range</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                        <span class="n">zoom_range</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
 
<span class="c1"># model parameters</span>
<span class="n">regularization</span> <span class="o">=</span> <span class="n">l2</span><span class="p">(</span><span class="n">l2_regularization</span><span class="p">)</span>
 
<span class="c1"># base</span>
<span class="n">img_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">img_input</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
 
<span class="c1"># module 1</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
 
<span class="c1"># module 2</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
 
<span class="c1"># module 3</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
 
<span class="c1"># module 4</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">residual</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">residual</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SeparableConv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">residual</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;predictions&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
 
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
 
<span class="c1"># callbacks</span>
<span class="n">log_file_path</span> <span class="o">=</span> <span class="n">base_path</span> <span class="o">+</span> <span class="s1">&#39;_emotion_training.log&#39;</span>
<span class="n">csv_logger</span> <span class="o">=</span> <span class="n">CSVLogger</span><span class="p">(</span><span class="n">log_file_path</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="n">patience</span><span class="p">)</span>
<span class="n">reduce_lr</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">patience</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">trained_models_path</span> <span class="o">=</span> <span class="n">base_path</span> <span class="o">+</span> <span class="s1">&#39;_mini_XCEPTION_New&#39;</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="n">trained_models_path</span> <span class="o">+</span> <span class="s1">&#39;.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_acc:.2f}</span><span class="s1">.hdf5&#39;</span>
<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_checkpoint</span><span class="p">,</span> <span class="n">csv_logger</span><span class="p">,</span> <span class="n">early_stop</span><span class="p">,</span> <span class="n">reduce_lr</span><span class="p">]</span>
 
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">data_generator</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span><span class="n">batch_size</span><span class="p">),</span>
                        <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: &#34;model_2&#34;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 48, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 46, 8)    72          input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 46, 46, 8)    32          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 46, 8)    0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 44, 44, 8)    576         activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 44, 44, 8)    32          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 44, 44, 8)    0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
separable_conv2d_9 (SeparableCo (None, 44, 44, 16)   200         activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_9[0][0]         
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 44, 44, 16)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
separable_conv2d_10 (SeparableC (None, 44, 44, 16)   400         activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_10[0][0]        
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 22, 22, 16)   128         activation_8[0][0]               
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 22, 22, 16)   64          conv2d_10[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_5[0][0]            
                                                                 batch_normalization_17[0][0]     
__________________________________________________________________________________________________
separable_conv2d_11 (SeparableC (None, 22, 22, 32)   656         add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_11[0][0]        
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 22, 22, 32)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
separable_conv2d_12 (SeparableC (None, 22, 22, 32)   1312        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_12[0][0]        
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 11, 11, 32)   512         add_5[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 11, 11, 32)   128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_6[0][0]            
                                                                 batch_normalization_20[0][0]     
__________________________________________________________________________________________________
separable_conv2d_13 (SeparableC (None, 11, 11, 64)   2336        add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_13[0][0]        
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 11, 11, 64)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
separable_conv2d_14 (SeparableC (None, 11, 11, 64)   4672        activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_14[0][0]        
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 6, 6, 64)     2048        add_6[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 6, 6, 64)     256         conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_7[0][0]            
                                                                 batch_normalization_23[0][0]     
__________________________________________________________________________________________________
separable_conv2d_15 (SeparableC (None, 6, 6, 128)    8768        add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_15[0][0]        
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 6, 6, 128)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
separable_conv2d_16 (SeparableC (None, 6, 6, 128)    17536       activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_16[0][0]        
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 3, 3, 128)    8192        add_7[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 3, 3, 128)    512         conv2d_13[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_8[0][0]            
                                                                 batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 3, 3, 7)      8071        add_8[0][0]                      
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 7)            0           conv2d_14[0][0]                  
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
Epoch 1/110
898/897 [==============================] - 50s 56ms/step - loss: 1.7892 - acc: 0.3188 - val_loss: 2.3508 - val_acc: 0.3164

Epoch 00001: val_loss improved from inf to 2.35076, saving model to _mini_XCEPTION_New.01-0.32.hdf5
Epoch 2/110
898/897 [==============================] - 47s 52ms/step - loss: 1.5330 - acc: 0.4238 - val_loss: 1.7992 - val_acc: 0.4154

Epoch 00002: val_loss improved from 2.35076 to 1.79922, saving model to _mini_XCEPTION_New.02-0.42.hdf5
Epoch 3/110
898/897 [==============================] - 46s 52ms/step - loss: 1.4176 - acc: 0.4669 - val_loss: 1.4200 - val_acc: 0.4794

Epoch 00003: val_loss improved from 1.79922 to 1.42004, saving model to _mini_XCEPTION_New.03-0.48.hdf5
Epoch 4/110
898/897 [==============================] - 46s 51ms/step - loss: 1.3499 - acc: 0.4928 - val_loss: 1.3638 - val_acc: 0.4808

Epoch 00004: val_loss improved from 1.42004 to 1.36380, saving model to _mini_XCEPTION_New.04-0.48.hdf5
Epoch 5/110
898/897 [==============================] - 46s 51ms/step - loss: 1.3084 - acc: 0.5067 - val_loss: 1.5857 - val_acc: 0.4362

Epoch 00005: val_loss did not improve from 1.36380
Epoch 6/110
898/897 [==============================] - 46s 51ms/step - loss: 1.2739 - acc: 0.5190 - val_loss: 1.4217 - val_acc: 0.4650

Epoch 00006: val_loss did not improve from 1.36380
Epoch 7/110
898/897 [==============================] - 45s 50ms/step - loss: 1.2492 - acc: 0.5278 - val_loss: 1.3696 - val_acc: 0.4900

Epoch 00007: val_loss did not improve from 1.36380
Epoch 8/110
898/897 [==============================] - 45s 51ms/step - loss: 1.2262 - acc: 0.5395 - val_loss: 1.3024 - val_acc: 0.5281

Epoch 00008: val_loss improved from 1.36380 to 1.30244, saving model to _mini_XCEPTION_New.08-0.53.hdf5
Epoch 9/110
898/897 [==============================] - 45s 50ms/step - loss: 1.2067 - acc: 0.5450 - val_loss: 1.2558 - val_acc: 0.5242

Epoch 00009: val_loss improved from 1.30244 to 1.25583, saving model to _mini_XCEPTION_New.09-0.52.hdf5
Epoch 10/110
898/897 [==============================] - 45s 51ms/step - loss: 1.1927 - acc: 0.5503 - val_loss: 1.2834 - val_acc: 0.5231

Epoch 00010: val_loss did not improve from 1.25583
Epoch 11/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1754 - acc: 0.5563 - val_loss: 1.1961 - val_acc: 0.5488

Epoch 00011: val_loss improved from 1.25583 to 1.19610, saving model to _mini_XCEPTION_New.11-0.55.hdf5
Epoch 12/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1624 - acc: 0.5645 - val_loss: 1.1805 - val_acc: 0.5581

Epoch 00012: val_loss improved from 1.19610 to 1.18050, saving model to _mini_XCEPTION_New.12-0.56.hdf5
Epoch 13/110
898/897 [==============================] - 46s 51ms/step - loss: 1.1531 - acc: 0.5659 - val_loss: 1.2436 - val_acc: 0.5371

Epoch 00013: val_loss did not improve from 1.18050
Epoch 14/110
898/897 [==============================] - 46s 51ms/step - loss: 1.1403 - acc: 0.5710 - val_loss: 1.2859 - val_acc: 0.5262

Epoch 00014: val_loss did not improve from 1.18050
Epoch 15/110
898/897 [==============================] - 46s 51ms/step - loss: 1.1304 - acc: 0.5758 - val_loss: 1.2058 - val_acc: 0.5539

Epoch 00015: val_loss did not improve from 1.18050
Epoch 16/110
898/897 [==============================] - 46s 51ms/step - loss: 1.1218 - acc: 0.5798 - val_loss: 1.1764 - val_acc: 0.5674

Epoch 00016: val_loss improved from 1.18050 to 1.17639, saving model to _mini_XCEPTION_New.16-0.57.hdf5
Epoch 17/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1170 - acc: 0.5825 - val_loss: 1.1680 - val_acc: 0.5591

Epoch 00017: val_loss improved from 1.17639 to 1.16800, saving model to _mini_XCEPTION_New.17-0.56.hdf5
Epoch 18/110
898/897 [==============================] - 46s 51ms/step - loss: 1.1132 - acc: 0.5813 - val_loss: 1.1330 - val_acc: 0.5759

Epoch 00018: val_loss improved from 1.16800 to 1.13302, saving model to _mini_XCEPTION_New.18-0.58.hdf5
Epoch 19/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1038 - acc: 0.5853 - val_loss: 1.1915 - val_acc: 0.5580

Epoch 00019: val_loss did not improve from 1.13302
Epoch 20/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0923 - acc: 0.5903 - val_loss: 1.2291 - val_acc: 0.5439

Epoch 00020: val_loss did not improve from 1.13302
Epoch 21/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0829 - acc: 0.5936 - val_loss: 1.1741 - val_acc: 0.5546

Epoch 00021: val_loss did not improve from 1.13302
Epoch 22/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0833 - acc: 0.5958 - val_loss: 1.2147 - val_acc: 0.5450

Epoch 00022: val_loss did not improve from 1.13302
Epoch 23/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0745 - acc: 0.5914 - val_loss: 1.1186 - val_acc: 0.5791

Epoch 00023: val_loss improved from 1.13302 to 1.11858, saving model to _mini_XCEPTION_New.23-0.58.hdf5
Epoch 24/110
898/897 [==============================] - 45s 51ms/step - loss: 1.0684 - acc: 0.5979 - val_loss: 1.1556 - val_acc: 0.5695

Epoch 00024: val_loss did not improve from 1.11858
Epoch 25/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0671 - acc: 0.6008 - val_loss: 1.1439 - val_acc: 0.5720

Epoch 00025: val_loss did not improve from 1.11858
Epoch 26/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0601 - acc: 0.6035 - val_loss: 1.0894 - val_acc: 0.5874

Epoch 00026: val_loss improved from 1.11858 to 1.08937, saving model to _mini_XCEPTION_New.26-0.59.hdf5
Epoch 27/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0573 - acc: 0.6042 - val_loss: 1.1797 - val_acc: 0.5593

Epoch 00027: val_loss did not improve from 1.08937
Epoch 28/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0564 - acc: 0.6027 - val_loss: 1.1290 - val_acc: 0.5858

Epoch 00028: val_loss did not improve from 1.08937
Epoch 29/110
898/897 [==============================] - 44s 50ms/step - loss: 1.0482 - acc: 0.6065 - val_loss: 1.1525 - val_acc: 0.5706

Epoch 00029: val_loss did not improve from 1.08937
Epoch 30/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0417 - acc: 0.6109 - val_loss: 1.0990 - val_acc: 0.5949

Epoch 00030: val_loss did not improve from 1.08937
Epoch 31/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0429 - acc: 0.6077 - val_loss: 1.0951 - val_acc: 0.5932

Epoch 00031: val_loss did not improve from 1.08937
Epoch 32/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0336 - acc: 0.6142 - val_loss: 1.1743 - val_acc: 0.5747

Epoch 00032: val_loss did not improve from 1.08937
Epoch 33/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0323 - acc: 0.6131 - val_loss: 1.1345 - val_acc: 0.5776

Epoch 00033: val_loss did not improve from 1.08937
Epoch 34/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0353 - acc: 0.6138 - val_loss: 1.1316 - val_acc: 0.5816

Epoch 00034: val_loss did not improve from 1.08937
Epoch 35/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0250 - acc: 0.6125 - val_loss: 1.1293 - val_acc: 0.5880

Epoch 00035: val_loss did not improve from 1.08937
Epoch 36/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0316 - acc: 0.6135 - val_loss: 1.2857 - val_acc: 0.5160

Epoch 00036: val_loss did not improve from 1.08937
Epoch 37/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0183 - acc: 0.6184 - val_loss: 1.1199 - val_acc: 0.5882

Epoch 00037: val_loss did not improve from 1.08937
Epoch 38/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0152 - acc: 0.6198 - val_loss: 1.1041 - val_acc: 0.5784

Epoch 00038: val_loss did not improve from 1.08937

Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 39/110
898/897 [==============================] - 44s 50ms/step - loss: 0.9697 - acc: 0.6363 - val_loss: 1.0504 - val_acc: 0.6156

Epoch 00039: val_loss improved from 1.08937 to 1.05035, saving model to _mini_XCEPTION_New.39-0.62.hdf5
Epoch 40/110
898/897 [==============================] - 44s 50ms/step - loss: 0.9580 - acc: 0.6428 - val_loss: 1.0401 - val_acc: 0.6170

Epoch 00040: val_loss improved from 1.05035 to 1.04014, saving model to _mini_XCEPTION_New.40-0.62.hdf5
Epoch 41/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9506 - acc: 0.6433 - val_loss: 1.0424 - val_acc: 0.6134

Epoch 00041: val_loss did not improve from 1.04014
Epoch 42/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9475 - acc: 0.6494 - val_loss: 1.0370 - val_acc: 0.6151

Epoch 00042: val_loss improved from 1.04014 to 1.03704, saving model to _mini_XCEPTION_New.42-0.62.hdf5
Epoch 43/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9458 - acc: 0.6479 - val_loss: 1.0420 - val_acc: 0.6123

Epoch 00043: val_loss did not improve from 1.03704
Epoch 44/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9407 - acc: 0.6465 - val_loss: 1.0474 - val_acc: 0.6108

Epoch 00044: val_loss did not improve from 1.03704
Epoch 45/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9405 - acc: 0.6505 - val_loss: 1.0467 - val_acc: 0.6149

Epoch 00045: val_loss did not improve from 1.03704
Epoch 46/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9326 - acc: 0.6514 - val_loss: 1.0333 - val_acc: 0.6204

Epoch 00046: val_loss improved from 1.03704 to 1.03328, saving model to _mini_XCEPTION_New.46-0.62.hdf5
Epoch 47/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9326 - acc: 0.6535 - val_loss: 1.0362 - val_acc: 0.6204

Epoch 00047: val_loss did not improve from 1.03328
Epoch 48/110
898/897 [==============================] - 46s 51ms/step - loss: 0.9273 - acc: 0.6527 - val_loss: 1.0345 - val_acc: 0.6201

Epoch 00048: val_loss did not improve from 1.03328
Epoch 49/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9324 - acc: 0.6526 - val_loss: 1.0382 - val_acc: 0.6220

Epoch 00049: val_loss did not improve from 1.03328
Epoch 50/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9278 - acc: 0.6555 - val_loss: 1.0379 - val_acc: 0.6180

Epoch 00050: val_loss did not improve from 1.03328
Epoch 51/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9261 - acc: 0.6541 - val_loss: 1.0375 - val_acc: 0.6197

Epoch 00051: val_loss did not improve from 1.03328
Epoch 52/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9208 - acc: 0.6541 - val_loss: 1.0344 - val_acc: 0.6222

Epoch 00052: val_loss did not improve from 1.03328
Epoch 53/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9206 - acc: 0.6556 - val_loss: 1.0388 - val_acc: 0.6255

Epoch 00053: val_loss did not improve from 1.03328
Epoch 54/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9240 - acc: 0.6538 - val_loss: 1.0382 - val_acc: 0.6201

Epoch 00054: val_loss did not improve from 1.03328
Epoch 55/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9208 - acc: 0.6549 - val_loss: 1.0422 - val_acc: 0.6180

Epoch 00055: val_loss did not improve from 1.03328
Epoch 56/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9223 - acc: 0.6548 - val_loss: 1.0358 - val_acc: 0.6233

Epoch 00056: val_loss did not improve from 1.03328
Epoch 57/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9209 - acc: 0.6568 - val_loss: 1.0590 - val_acc: 0.6159

Epoch 00057: val_loss did not improve from 1.03328
Epoch 58/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9200 - acc: 0.6581 - val_loss: 1.0408 - val_acc: 0.6223

Epoch 00058: val_loss did not improve from 1.03328

Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 59/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9114 - acc: 0.6605 - val_loss: 1.0361 - val_acc: 0.6251

Epoch 00059: val_loss did not improve from 1.03328
Epoch 60/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9174 - acc: 0.6579 - val_loss: 1.0340 - val_acc: 0.6258

Epoch 00060: val_loss did not improve from 1.03328
Epoch 61/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9133 - acc: 0.6576 - val_loss: 1.0329 - val_acc: 0.6248

Epoch 00061: val_loss improved from 1.03328 to 1.03289, saving model to _mini_XCEPTION_New.61-0.62.hdf5
Epoch 62/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9122 - acc: 0.6595 - val_loss: 1.0334 - val_acc: 0.6262

Epoch 00062: val_loss did not improve from 1.03289
Epoch 63/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9042 - acc: 0.6609 - val_loss: 1.0314 - val_acc: 0.6262

Epoch 00063: val_loss improved from 1.03289 to 1.03139, saving model to _mini_XCEPTION_New.63-0.63.hdf5
Epoch 64/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9105 - acc: 0.6616 - val_loss: 1.0317 - val_acc: 0.6265

Epoch 00064: val_loss did not improve from 1.03139
Epoch 65/110
898/897 [==============================] - 42s 47ms/step - loss: 0.9122 - acc: 0.6620 - val_loss: 1.0329 - val_acc: 0.6280

Epoch 00065: val_loss did not improve from 1.03139
Epoch 66/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9139 - acc: 0.6581 - val_loss: 1.0318 - val_acc: 0.6261

Epoch 00066: val_loss did not improve from 1.03139
Epoch 67/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9091 - acc: 0.6597 - val_loss: 1.0334 - val_acc: 0.6265

Epoch 00067: val_loss did not improve from 1.03139
Epoch 68/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9064 - acc: 0.6631 - val_loss: 1.0326 - val_acc: 0.6254

Epoch 00068: val_loss did not improve from 1.03139
Epoch 69/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9050 - acc: 0.6631 - val_loss: 1.0317 - val_acc: 0.6251

Epoch 00069: val_loss did not improve from 1.03139
Epoch 70/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9057 - acc: 0.6618 - val_loss: 1.0347 - val_acc: 0.6254

Epoch 00070: val_loss did not improve from 1.03139
Epoch 71/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9044 - acc: 0.6628 - val_loss: 1.0324 - val_acc: 0.6259

Epoch 00071: val_loss did not improve from 1.03139
Epoch 72/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9087 - acc: 0.6595 - val_loss: 1.0321 - val_acc: 0.6255

Epoch 00072: val_loss did not improve from 1.03139
Epoch 73/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9078 - acc: 0.6639 - val_loss: 1.0306 - val_acc: 0.6269

Epoch 00073: val_loss improved from 1.03139 to 1.03059, saving model to _mini_XCEPTION_New.73-0.63.hdf5
Epoch 74/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9092 - acc: 0.6583 - val_loss: 1.0321 - val_acc: 0.6262

Epoch 00074: val_loss did not improve from 1.03059
Epoch 75/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9042 - acc: 0.6632 - val_loss: 1.0321 - val_acc: 0.6248

Epoch 00075: val_loss did not improve from 1.03059
Epoch 76/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9073 - acc: 0.6605 - val_loss: 1.0318 - val_acc: 0.6257

Epoch 00076: val_loss did not improve from 1.03059
Epoch 77/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9063 - acc: 0.6631 - val_loss: 1.0319 - val_acc: 0.6252

Epoch 00077: val_loss did not improve from 1.03059
Epoch 78/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9106 - acc: 0.6595 - val_loss: 1.0317 - val_acc: 0.6272

Epoch 00078: val_loss did not improve from 1.03059
Epoch 79/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9045 - acc: 0.6615 - val_loss: 1.0298 - val_acc: 0.6262

Epoch 00079: val_loss improved from 1.03059 to 1.02985, saving model to _mini_XCEPTION_New.79-0.63.hdf5
Epoch 80/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9034 - acc: 0.6621 - val_loss: 1.0325 - val_acc: 0.6252

Epoch 00080: val_loss did not improve from 1.02985
Epoch 81/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9080 - acc: 0.6621 - val_loss: 1.0323 - val_acc: 0.6247

Epoch 00081: val_loss did not improve from 1.02985
Epoch 82/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9048 - acc: 0.6627 - val_loss: 1.0311 - val_acc: 0.6262

Epoch 00082: val_loss did not improve from 1.02985
Epoch 83/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9089 - acc: 0.6591 - val_loss: 1.0310 - val_acc: 0.6255

Epoch 00083: val_loss did not improve from 1.02985
Epoch 84/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9127 - acc: 0.6585 - val_loss: 1.0309 - val_acc: 0.6254

Epoch 00084: val_loss did not improve from 1.02985
Epoch 85/110
898/897 [==============================] - 43s 47ms/step - loss: 0.9114 - acc: 0.6598 - val_loss: 1.0307 - val_acc: 0.6276

Epoch 00085: val_loss did not improve from 1.02985
Epoch 86/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9061 - acc: 0.6629 - val_loss: 1.0317 - val_acc: 0.6265

Epoch 00086: val_loss did not improve from 1.02985
Epoch 87/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9031 - acc: 0.6626 - val_loss: 1.0322 - val_acc: 0.6255

Epoch 00087: val_loss did not improve from 1.02985
Epoch 88/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9113 - acc: 0.6632 - val_loss: 1.0316 - val_acc: 0.6271

Epoch 00088: val_loss did not improve from 1.02985
Epoch 89/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9074 - acc: 0.6604 - val_loss: 1.0312 - val_acc: 0.6254

Epoch 00089: val_loss did not improve from 1.02985
Epoch 90/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9051 - acc: 0.6606 - val_loss: 1.0310 - val_acc: 0.6258

Epoch 00090: val_loss did not improve from 1.02985
Epoch 91/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9078 - acc: 0.6625 - val_loss: 1.0315 - val_acc: 0.6252

Epoch 00091: val_loss did not improve from 1.02985

Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Epoch 92/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9101 - acc: 0.6601 - val_loss: 1.0311 - val_acc: 0.6264

Epoch 00092: val_loss did not improve from 1.02985
Epoch 93/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9034 - acc: 0.6619 - val_loss: 1.0308 - val_acc: 0.6257

Epoch 00093: val_loss did not improve from 1.02985
Epoch 94/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9074 - acc: 0.6618 - val_loss: 1.0301 - val_acc: 0.6264

Epoch 00094: val_loss did not improve from 1.02985
Epoch 95/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9065 - acc: 0.6617 - val_loss: 1.0309 - val_acc: 0.6262

Epoch 00095: val_loss did not improve from 1.02985
Epoch 96/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9040 - acc: 0.6615 - val_loss: 1.0300 - val_acc: 0.6252

Epoch 00096: val_loss did not improve from 1.02985
Epoch 97/110
898/897 [==============================] - 43s 48ms/step - loss: 0.8988 - acc: 0.6645 - val_loss: 1.0306 - val_acc: 0.6265

Epoch 00097: val_loss did not improve from 1.02985
Epoch 98/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9023 - acc: 0.6649 - val_loss: 1.0314 - val_acc: 0.6258

Epoch 00098: val_loss did not improve from 1.02985
Epoch 99/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9032 - acc: 0.6623 - val_loss: 1.0310 - val_acc: 0.6255

Epoch 00099: val_loss did not improve from 1.02985
Epoch 100/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9010 - acc: 0.6662 - val_loss: 1.0316 - val_acc: 0.6250

Epoch 00100: val_loss did not improve from 1.02985
Epoch 101/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9047 - acc: 0.6602 - val_loss: 1.0312 - val_acc: 0.6259

Epoch 00101: val_loss did not improve from 1.02985
Epoch 102/110
898/897 [==============================] - 43s 48ms/step - loss: 0.9006 - acc: 0.6631 - val_loss: 1.0305 - val_acc: 0.6264

Epoch 00102: val_loss did not improve from 1.02985
Epoch 103/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9010 - acc: 0.6665 - val_loss: 1.0321 - val_acc: 0.6261

Epoch 00103: val_loss did not improve from 1.02985

Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
Epoch 104/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8989 - acc: 0.6648 - val_loss: 1.0313 - val_acc: 0.6259

Epoch 00104: val_loss did not improve from 1.02985
Epoch 105/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9055 - acc: 0.6630 - val_loss: 1.0310 - val_acc: 0.6269

Epoch 00105: val_loss did not improve from 1.02985
Epoch 106/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9051 - acc: 0.6645 - val_loss: 1.0316 - val_acc: 0.6255

Epoch 00106: val_loss did not improve from 1.02985
Epoch 107/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8966 - acc: 0.6671 - val_loss: 1.0314 - val_acc: 0.6269

Epoch 00107: val_loss did not improve from 1.02985
Epoch 108/110
898/897 [==============================] - 45s 50ms/step - loss: 0.8992 - acc: 0.6665 - val_loss: 1.0313 - val_acc: 0.6262

Epoch 00108: val_loss did not improve from 1.02985
Epoch 109/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9075 - acc: 0.6613 - val_loss: 1.0313 - val_acc: 0.6254

Epoch 00109: val_loss did not improve from 1.02985
Epoch 110/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9062 - acc: 0.6643 - val_loss: 1.0317 - val_acc: 0.6259

Epoch 00110: val_loss did not improve from 1.02985
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;keras.callbacks.History at 0x7fd678d03f60&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gainClassification_report</span><span class="p">(</span><span class="n">emotion_classifier</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-90-07971fdd68f8&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>gainClassification_report<span class="ansi-blue-fg">(</span>emotion_classifier<span class="ansi-blue-fg">,</span>xtest<span class="ansi-blue-fg">,</span>ytest<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;emotion_classifier&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Confusion-Matrix">Confusion Matrix<a class="anchor-link" href="#Confusion-Matrix">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">classification_report</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">gainClassification_report</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">):</span>
  <span class="n">EMOTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;angry&quot;</span><span class="p">,</span><span class="s2">&quot;disgust&quot;</span><span class="p">,</span><span class="s2">&quot;fear&quot;</span><span class="p">,</span> <span class="s2">&quot;happy&quot;</span><span class="p">,</span> <span class="s2">&quot;sad&quot;</span><span class="p">,</span> <span class="s2">&quot;surprised&quot;</span><span class="p">,</span><span class="s2">&quot;neutral&quot;</span><span class="p">]</span>
  <span class="n">ypred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
  <span class="n">matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">ypred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>

  <span class="n">y_test</span> <span class="o">=</span> <span class="n">ytest</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">ypred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification Report&#39;</span><span class="p">)</span>
  <span class="n">target_names</span> <span class="o">=</span> <span class="n">EMOTIONS</span>
  <span class="n">report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">matrix</span><span class="p">,</span><span class="n">report</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gainClassification_report</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 649   15   58   37   84   26  122]
 [  23   67    6    0    8    4    1]
 [ 207    8  415   44  114   94  142]
 [  48    2   28 1577   21   40   82]
 [ 182    3  130   73  500   29  299]
 [  34    3   83   43    4  597   36]
 [  85    3   47  108  111   26  860]]
Classification Report
              precision    recall  f1-score   support

       angry       0.53      0.65      0.58       991
     disgust       0.66      0.61      0.64       109
        fear       0.54      0.41      0.46      1024
       happy       0.84      0.88      0.86      1798
         sad       0.59      0.41      0.49      1216
   surprised       0.73      0.75      0.74       800
     neutral       0.56      0.69      0.62      1240

    accuracy                           0.65      7178
   macro avg       0.64      0.63      0.63      7178
weighted avg       0.65      0.65      0.64      7178

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(array([[ 649,   15,   58,   37,   84,   26,  122],
        [  23,   67,    6,    0,    8,    4,    1],
        [ 207,    8,  415,   44,  114,   94,  142],
        [  48,    2,   28, 1577,   21,   40,   82],
        [ 182,    3,  130,   73,  500,   29,  299],
        [  34,    3,   83,   43,    4,  597,   36],
        [  85,    3,   47,  108,  111,   26,  860]]),
 &#39;              precision    recall  f1-score   support\n\n       angry       0.53      0.65      0.58       991\n     disgust       0.66      0.61      0.64       109\n        fear       0.54      0.41      0.46      1024\n       happy       0.84      0.88      0.86      1798\n         sad       0.59      0.41      0.49      1216\n   surprised       0.73      0.75      0.74       800\n     neutral       0.56      0.69      0.62      1240\n\n    accuracy                           0.65      7178\n   macro avg       0.64      0.63      0.63      7178\nweighted avg       0.65      0.65      0.64      7178\n&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-Training:-Only-GCN">Model Training: Only GCN<a class="anchor-link" href="#Model-Training:-Only-GCN">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">trainCNN</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: &#34;model_4&#34;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 48, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 46, 46, 8)    72          input_4[0][0]                    
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 46, 46, 8)    32          conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 46, 46, 8)    0           batch_normalization_43[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 44, 44, 8)    576         activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 44, 44, 8)    32          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 44, 44, 8)    0           batch_normalization_44[0][0]     
__________________________________________________________________________________________________
separable_conv2d_25 (SeparableC (None, 44, 44, 16)   200         activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_25[0][0]        
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 44, 44, 16)   0           batch_normalization_46[0][0]     
__________________________________________________________________________________________________
separable_conv2d_26 (SeparableC (None, 44, 44, 16)   400         activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_26[0][0]        
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 22, 22, 16)   128         activation_20[0][0]              
__________________________________________________________________________________________________
max_pooling2d_13 (MaxPooling2D) (None, 22, 22, 16)   0           batch_normalization_47[0][0]     
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 22, 22, 16)   64          conv2d_24[0][0]                  
__________________________________________________________________________________________________
add_13 (Add)                    (None, 22, 22, 16)   0           max_pooling2d_13[0][0]           
                                                                 batch_normalization_45[0][0]     
__________________________________________________________________________________________________
separable_conv2d_27 (SeparableC (None, 22, 22, 32)   656         add_13[0][0]                     
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_27[0][0]        
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 22, 22, 32)   0           batch_normalization_49[0][0]     
__________________________________________________________________________________________________
separable_conv2d_28 (SeparableC (None, 22, 22, 32)   1312        activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_28[0][0]        
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 11, 11, 32)   512         add_13[0][0]                     
__________________________________________________________________________________________________
max_pooling2d_14 (MaxPooling2D) (None, 11, 11, 32)   0           batch_normalization_50[0][0]     
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 11, 11, 32)   128         conv2d_25[0][0]                  
__________________________________________________________________________________________________
add_14 (Add)                    (None, 11, 11, 32)   0           max_pooling2d_14[0][0]           
                                                                 batch_normalization_48[0][0]     
__________________________________________________________________________________________________
separable_conv2d_29 (SeparableC (None, 11, 11, 64)   2336        add_14[0][0]                     
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_29[0][0]        
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 11, 11, 64)   0           batch_normalization_52[0][0]     
__________________________________________________________________________________________________
separable_conv2d_30 (SeparableC (None, 11, 11, 64)   4672        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_30[0][0]        
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 6, 6, 64)     2048        add_14[0][0]                     
__________________________________________________________________________________________________
max_pooling2d_15 (MaxPooling2D) (None, 6, 6, 64)     0           batch_normalization_53[0][0]     
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 6, 6, 64)     256         conv2d_26[0][0]                  
__________________________________________________________________________________________________
add_15 (Add)                    (None, 6, 6, 64)     0           max_pooling2d_15[0][0]           
                                                                 batch_normalization_51[0][0]     
__________________________________________________________________________________________________
separable_conv2d_31 (SeparableC (None, 6, 6, 128)    8768        add_15[0][0]                     
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_31[0][0]        
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 6, 6, 128)    0           batch_normalization_55[0][0]     
__________________________________________________________________________________________________
separable_conv2d_32 (SeparableC (None, 6, 6, 128)    17536       activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_32[0][0]        
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 3, 3, 128)    8192        add_15[0][0]                     
__________________________________________________________________________________________________
max_pooling2d_16 (MaxPooling2D) (None, 3, 3, 128)    0           batch_normalization_56[0][0]     
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 3, 3, 128)    512         conv2d_27[0][0]                  
__________________________________________________________________________________________________
add_16 (Add)                    (None, 3, 3, 128)    0           max_pooling2d_16[0][0]           
                                                                 batch_normalization_54[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 3, 3, 7)      8071        add_16[0][0]                     
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 7)            0           conv2d_28[0][0]                  
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
Epoch 1/110
898/897 [==============================] - 50s 56ms/step - loss: 1.7362 - acc: 0.3418 - val_loss: 1.6615 - val_acc: 0.3943

Epoch 00001: val_loss improved from inf to 1.66152, saving model to models_GCN/_mini_XCEPTION_GCN.01-0.39.hdf5
Epoch 2/110
898/897 [==============================] - 45s 50ms/step - loss: 1.4976 - acc: 0.4378 - val_loss: 1.5842 - val_acc: 0.4221

Epoch 00002: val_loss improved from 1.66152 to 1.58423, saving model to models_GCN/_mini_XCEPTION_GCN.02-0.42.hdf5
Epoch 3/110
898/897 [==============================] - 44s 50ms/step - loss: 1.3966 - acc: 0.4742 - val_loss: 1.4309 - val_acc: 0.4636

Epoch 00003: val_loss improved from 1.58423 to 1.43093, saving model to models_GCN/_mini_XCEPTION_GCN.03-0.46.hdf5
Epoch 4/110
898/897 [==============================] - 45s 50ms/step - loss: 1.3357 - acc: 0.4983 - val_loss: 1.6493 - val_acc: 0.3844

Epoch 00004: val_loss did not improve from 1.43093
Epoch 5/110
898/897 [==============================] - 44s 50ms/step - loss: 1.2960 - acc: 0.5088 - val_loss: 1.3356 - val_acc: 0.5056

Epoch 00005: val_loss improved from 1.43093 to 1.33558, saving model to models_GCN/_mini_XCEPTION_GCN.05-0.51.hdf5
Epoch 6/110
898/897 [==============================] - 44s 49ms/step - loss: 1.2599 - acc: 0.5256 - val_loss: 1.3006 - val_acc: 0.5160

Epoch 00006: val_loss improved from 1.33558 to 1.30060, saving model to models_GCN/_mini_XCEPTION_GCN.06-0.52.hdf5
Epoch 7/110
898/897 [==============================] - 44s 49ms/step - loss: 1.2356 - acc: 0.5330 - val_loss: 1.3113 - val_acc: 0.5131

Epoch 00007: val_loss did not improve from 1.30060
Epoch 8/110
898/897 [==============================] - 44s 49ms/step - loss: 1.2191 - acc: 0.5400 - val_loss: 1.2828 - val_acc: 0.5270

Epoch 00008: val_loss improved from 1.30060 to 1.28281, saving model to models_GCN/_mini_XCEPTION_GCN.08-0.53.hdf5
Epoch 9/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1987 - acc: 0.5480 - val_loss: 1.3078 - val_acc: 0.5128

Epoch 00009: val_loss did not improve from 1.28281
Epoch 10/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1853 - acc: 0.5576 - val_loss: 1.2297 - val_acc: 0.5444

Epoch 00010: val_loss improved from 1.28281 to 1.22973, saving model to models_GCN/_mini_XCEPTION_GCN.10-0.54.hdf5
Epoch 11/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1645 - acc: 0.5597 - val_loss: 1.2846 - val_acc: 0.5359

Epoch 00011: val_loss did not improve from 1.22973
Epoch 12/110
898/897 [==============================] - 44s 49ms/step - loss: 1.1590 - acc: 0.5638 - val_loss: 1.3040 - val_acc: 0.5219

Epoch 00012: val_loss did not improve from 1.22973
Epoch 13/110
898/897 [==============================] - 44s 49ms/step - loss: 1.1465 - acc: 0.5707 - val_loss: 1.3039 - val_acc: 0.5220

Epoch 00013: val_loss did not improve from 1.22973
Epoch 14/110
898/897 [==============================] - 44s 49ms/step - loss: 1.1344 - acc: 0.5754 - val_loss: 1.1815 - val_acc: 0.5593

Epoch 00014: val_loss improved from 1.22973 to 1.18146, saving model to models_GCN/_mini_XCEPTION_GCN.14-0.56.hdf5
Epoch 15/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1289 - acc: 0.5758 - val_loss: 1.4845 - val_acc: 0.4575

Epoch 00015: val_loss did not improve from 1.18146
Epoch 16/110
898/897 [==============================] - 44s 49ms/step - loss: 1.1171 - acc: 0.5801 - val_loss: 1.2054 - val_acc: 0.5545

Epoch 00016: val_loss did not improve from 1.18146
Epoch 17/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1054 - acc: 0.5893 - val_loss: 1.2715 - val_acc: 0.5426

Epoch 00017: val_loss did not improve from 1.18146
Epoch 18/110
898/897 [==============================] - 45s 50ms/step - loss: 1.1002 - acc: 0.5889 - val_loss: 1.1929 - val_acc: 0.5642

Epoch 00018: val_loss did not improve from 1.18146
Epoch 19/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0968 - acc: 0.5880 - val_loss: 1.1537 - val_acc: 0.5666

Epoch 00019: val_loss improved from 1.18146 to 1.15368, saving model to models_GCN/_mini_XCEPTION_GCN.19-0.57.hdf5
Epoch 20/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0858 - acc: 0.5897 - val_loss: 1.1222 - val_acc: 0.5886

Epoch 00020: val_loss improved from 1.15368 to 1.12225, saving model to models_GCN/_mini_XCEPTION_GCN.20-0.59.hdf5
Epoch 21/110
898/897 [==============================] - 44s 49ms/step - loss: 1.0814 - acc: 0.5977 - val_loss: 1.1820 - val_acc: 0.5651

Epoch 00021: val_loss did not improve from 1.12225
Epoch 22/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0774 - acc: 0.5974 - val_loss: 1.2337 - val_acc: 0.5475

Epoch 00022: val_loss did not improve from 1.12225
Epoch 23/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0671 - acc: 0.6016 - val_loss: 1.1287 - val_acc: 0.5876

Epoch 00023: val_loss did not improve from 1.12225
Epoch 24/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0648 - acc: 0.6032 - val_loss: 1.1605 - val_acc: 0.5716

Epoch 00024: val_loss did not improve from 1.12225
Epoch 25/110
898/897 [==============================] - 46s 52ms/step - loss: 1.0611 - acc: 0.6050 - val_loss: 1.1710 - val_acc: 0.5667

Epoch 00025: val_loss did not improve from 1.12225
Epoch 26/110
898/897 [==============================] - 48s 53ms/step - loss: 1.0576 - acc: 0.6068 - val_loss: 1.1423 - val_acc: 0.5731

Epoch 00026: val_loss did not improve from 1.12225
Epoch 27/110
898/897 [==============================] - 46s 52ms/step - loss: 1.0442 - acc: 0.6117 - val_loss: 1.1325 - val_acc: 0.5809

Epoch 00027: val_loss did not improve from 1.12225
Epoch 28/110
898/897 [==============================] - 46s 51ms/step - loss: 1.0356 - acc: 0.6107 - val_loss: 1.1404 - val_acc: 0.5772

Epoch 00028: val_loss did not improve from 1.12225
Epoch 29/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0365 - acc: 0.6129 - val_loss: 1.1433 - val_acc: 0.5772

Epoch 00029: val_loss did not improve from 1.12225
Epoch 30/110
898/897 [==============================] - 46s 51ms/step - loss: 1.0325 - acc: 0.6122 - val_loss: 1.1482 - val_acc: 0.5773

Epoch 00030: val_loss did not improve from 1.12225
Epoch 31/110
898/897 [==============================] - 45s 50ms/step - loss: 1.0307 - acc: 0.6117 - val_loss: 1.1658 - val_acc: 0.5776

Epoch 00031: val_loss did not improve from 1.12225
Epoch 32/110
898/897 [==============================] - 48s 53ms/step - loss: 1.0295 - acc: 0.6195 - val_loss: 1.1175 - val_acc: 0.5875

Epoch 00032: val_loss improved from 1.12225 to 1.11752, saving model to models_GCN/_mini_XCEPTION_GCN.32-0.59.hdf5
Epoch 33/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0242 - acc: 0.6184 - val_loss: 1.0998 - val_acc: 0.5918

Epoch 00033: val_loss improved from 1.11752 to 1.09979, saving model to models_GCN/_mini_XCEPTION_GCN.33-0.59.hdf5
Epoch 34/110
898/897 [==============================] - 47s 53ms/step - loss: 1.0194 - acc: 0.6169 - val_loss: 1.1172 - val_acc: 0.5858

Epoch 00034: val_loss did not improve from 1.09979
Epoch 35/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0222 - acc: 0.6162 - val_loss: 1.1499 - val_acc: 0.5715

Epoch 00035: val_loss did not improve from 1.09979
Epoch 36/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0129 - acc: 0.6198 - val_loss: 1.1392 - val_acc: 0.5879

Epoch 00036: val_loss did not improve from 1.09979
Epoch 37/110
898/897 [==============================] - 47s 53ms/step - loss: 1.0113 - acc: 0.6219 - val_loss: 1.0876 - val_acc: 0.5984

Epoch 00037: val_loss improved from 1.09979 to 1.08758, saving model to models_GCN/_mini_XCEPTION_GCN.37-0.60.hdf5
Epoch 38/110
898/897 [==============================] - 47s 53ms/step - loss: 1.0050 - acc: 0.6230 - val_loss: 1.1478 - val_acc: 0.5706

Epoch 00038: val_loss did not improve from 1.08758
Epoch 39/110
898/897 [==============================] - 48s 54ms/step - loss: 1.0084 - acc: 0.6259 - val_loss: 1.1810 - val_acc: 0.5674

Epoch 00039: val_loss did not improve from 1.08758
Epoch 40/110
898/897 [==============================] - 47s 53ms/step - loss: 1.0069 - acc: 0.6248 - val_loss: 1.2070 - val_acc: 0.5616

Epoch 00040: val_loss did not improve from 1.08758
Epoch 41/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9955 - acc: 0.6275 - val_loss: 1.1308 - val_acc: 0.5899

Epoch 00041: val_loss did not improve from 1.08758
Epoch 42/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9961 - acc: 0.6272 - val_loss: 1.1057 - val_acc: 0.5975

Epoch 00042: val_loss did not improve from 1.08758
Epoch 43/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9904 - acc: 0.6317 - val_loss: 1.0866 - val_acc: 0.5975

Epoch 00043: val_loss improved from 1.08758 to 1.08664, saving model to models_GCN/_mini_XCEPTION_GCN.43-0.60.hdf5
Epoch 44/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9896 - acc: 0.6314 - val_loss: 1.0737 - val_acc: 0.6017

Epoch 00044: val_loss improved from 1.08664 to 1.07373, saving model to models_GCN/_mini_XCEPTION_GCN.44-0.60.hdf5
Epoch 45/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9891 - acc: 0.6303 - val_loss: 1.1441 - val_acc: 0.5836

Epoch 00045: val_loss did not improve from 1.07373
Epoch 46/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9832 - acc: 0.6334 - val_loss: 1.1168 - val_acc: 0.5975

Epoch 00046: val_loss did not improve from 1.07373
Epoch 47/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9838 - acc: 0.6333 - val_loss: 1.1569 - val_acc: 0.5763

Epoch 00047: val_loss did not improve from 1.07373
Epoch 48/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9813 - acc: 0.6360 - val_loss: 1.1814 - val_acc: 0.5720

Epoch 00048: val_loss did not improve from 1.07373
Epoch 49/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9766 - acc: 0.6362 - val_loss: 1.1378 - val_acc: 0.5809

Epoch 00049: val_loss did not improve from 1.07373
Epoch 50/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9775 - acc: 0.6361 - val_loss: 1.1429 - val_acc: 0.5933

Epoch 00050: val_loss did not improve from 1.07373
Epoch 51/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9711 - acc: 0.6399 - val_loss: 1.0728 - val_acc: 0.6060

Epoch 00051: val_loss improved from 1.07373 to 1.07283, saving model to models_GCN/_mini_XCEPTION_GCN.51-0.61.hdf5
Epoch 52/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9722 - acc: 0.6399 - val_loss: 1.1011 - val_acc: 0.6030

Epoch 00052: val_loss did not improve from 1.07283
Epoch 53/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9722 - acc: 0.6351 - val_loss: 1.0856 - val_acc: 0.6042

Epoch 00053: val_loss did not improve from 1.07283
Epoch 54/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9696 - acc: 0.6368 - val_loss: 1.0719 - val_acc: 0.6000

Epoch 00054: val_loss improved from 1.07283 to 1.07191, saving model to models_GCN/_mini_XCEPTION_GCN.54-0.60.hdf5
Epoch 55/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9640 - acc: 0.6376 - val_loss: 1.1532 - val_acc: 0.5737

Epoch 00055: val_loss did not improve from 1.07191
Epoch 56/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9639 - acc: 0.6386 - val_loss: 1.0727 - val_acc: 0.6053

Epoch 00056: val_loss did not improve from 1.07191
Epoch 57/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9552 - acc: 0.6440 - val_loss: 1.0681 - val_acc: 0.6070

Epoch 00057: val_loss improved from 1.07191 to 1.06810, saving model to models_GCN/_mini_XCEPTION_GCN.57-0.61.hdf5
Epoch 58/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9566 - acc: 0.6425 - val_loss: 1.0608 - val_acc: 0.6069

Epoch 00058: val_loss improved from 1.06810 to 1.06080, saving model to models_GCN/_mini_XCEPTION_GCN.58-0.61.hdf5
Epoch 59/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9566 - acc: 0.6433 - val_loss: 1.0732 - val_acc: 0.6006

Epoch 00059: val_loss did not improve from 1.06080
Epoch 60/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9582 - acc: 0.6419 - val_loss: 1.0632 - val_acc: 0.6037

Epoch 00060: val_loss did not improve from 1.06080
Epoch 61/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9532 - acc: 0.6450 - val_loss: 1.0662 - val_acc: 0.6147

Epoch 00061: val_loss did not improve from 1.06080
Epoch 62/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9517 - acc: 0.6423 - val_loss: 1.0974 - val_acc: 0.6023

Epoch 00062: val_loss did not improve from 1.06080
Epoch 63/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9443 - acc: 0.6488 - val_loss: 1.1027 - val_acc: 0.5940

Epoch 00063: val_loss did not improve from 1.06080
Epoch 64/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9488 - acc: 0.6460 - val_loss: 1.0996 - val_acc: 0.6007

Epoch 00064: val_loss did not improve from 1.06080
Epoch 65/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9431 - acc: 0.6506 - val_loss: 1.0973 - val_acc: 0.5997

Epoch 00065: val_loss did not improve from 1.06080
Epoch 66/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9384 - acc: 0.6500 - val_loss: 1.0984 - val_acc: 0.6009

Epoch 00066: val_loss did not improve from 1.06080
Epoch 67/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9399 - acc: 0.6486 - val_loss: 1.0575 - val_acc: 0.6140

Epoch 00067: val_loss improved from 1.06080 to 1.05748, saving model to models_GCN/_mini_XCEPTION_GCN.67-0.61.hdf5
Epoch 68/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9368 - acc: 0.6504 - val_loss: 1.0889 - val_acc: 0.5992

Epoch 00068: val_loss did not improve from 1.05748
Epoch 69/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9374 - acc: 0.6512 - val_loss: 1.1253 - val_acc: 0.5809

Epoch 00069: val_loss did not improve from 1.05748
Epoch 70/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9307 - acc: 0.6547 - val_loss: 1.0941 - val_acc: 0.6030

Epoch 00070: val_loss did not improve from 1.05748
Epoch 71/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9325 - acc: 0.6560 - val_loss: 1.0817 - val_acc: 0.6024

Epoch 00071: val_loss did not improve from 1.05748
Epoch 72/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9346 - acc: 0.6511 - val_loss: 1.0941 - val_acc: 0.6041

Epoch 00072: val_loss did not improve from 1.05748
Epoch 73/110
898/897 [==============================] - 48s 53ms/step - loss: 0.9341 - acc: 0.6543 - val_loss: 1.1520 - val_acc: 0.5968

Epoch 00073: val_loss did not improve from 1.05748
Epoch 74/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9343 - acc: 0.6505 - val_loss: 1.0994 - val_acc: 0.6056

Epoch 00074: val_loss did not improve from 1.05748
Epoch 75/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9279 - acc: 0.6557 - val_loss: 1.1198 - val_acc: 0.5996

Epoch 00075: val_loss did not improve from 1.05748
Epoch 76/110
898/897 [==============================] - 47s 53ms/step - loss: 0.9238 - acc: 0.6570 - val_loss: 1.0788 - val_acc: 0.6080

Epoch 00076: val_loss did not improve from 1.05748
Epoch 77/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9318 - acc: 0.6545 - val_loss: 1.1076 - val_acc: 0.5896

Epoch 00077: val_loss did not improve from 1.05748
Epoch 78/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9227 - acc: 0.6547 - val_loss: 1.0978 - val_acc: 0.6030

Epoch 00078: val_loss did not improve from 1.05748
Epoch 79/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9208 - acc: 0.6571 - val_loss: 1.0766 - val_acc: 0.6074

Epoch 00079: val_loss did not improve from 1.05748

Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 80/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8792 - acc: 0.6731 - val_loss: 1.0307 - val_acc: 0.6195

Epoch 00080: val_loss improved from 1.05748 to 1.03067, saving model to models_GCN/_mini_XCEPTION_GCN.80-0.62.hdf5
Epoch 81/110
898/897 [==============================] - 50s 55ms/step - loss: 0.8615 - acc: 0.6803 - val_loss: 1.0334 - val_acc: 0.6199

Epoch 00081: val_loss did not improve from 1.03067
Epoch 82/110
898/897 [==============================] - 49s 55ms/step - loss: 0.8602 - acc: 0.6807 - val_loss: 1.0287 - val_acc: 0.6219

Epoch 00082: val_loss improved from 1.03067 to 1.02866, saving model to models_GCN/_mini_XCEPTION_GCN.82-0.62.hdf5
Epoch 83/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8510 - acc: 0.6839 - val_loss: 1.0293 - val_acc: 0.6234

Epoch 00083: val_loss did not improve from 1.02866
Epoch 84/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8530 - acc: 0.6854 - val_loss: 1.0279 - val_acc: 0.6245

Epoch 00084: val_loss improved from 1.02866 to 1.02792, saving model to models_GCN/_mini_XCEPTION_GCN.84-0.62.hdf5
Epoch 85/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8519 - acc: 0.6831 - val_loss: 1.0298 - val_acc: 0.6234

Epoch 00085: val_loss did not improve from 1.02792
Epoch 86/110
898/897 [==============================] - 47s 53ms/step - loss: 0.8502 - acc: 0.6865 - val_loss: 1.0267 - val_acc: 0.6230

Epoch 00086: val_loss improved from 1.02792 to 1.02670, saving model to models_GCN/_mini_XCEPTION_GCN.86-0.62.hdf5
Epoch 87/110
898/897 [==============================] - 46s 51ms/step - loss: 0.8474 - acc: 0.6880 - val_loss: 1.0280 - val_acc: 0.6258

Epoch 00087: val_loss did not improve from 1.02670
Epoch 88/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8417 - acc: 0.6860 - val_loss: 1.0339 - val_acc: 0.6218

Epoch 00088: val_loss did not improve from 1.02670
Epoch 89/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8459 - acc: 0.6870 - val_loss: 1.0284 - val_acc: 0.6176

Epoch 00089: val_loss did not improve from 1.02670
Epoch 90/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8430 - acc: 0.6898 - val_loss: 1.0324 - val_acc: 0.6223

Epoch 00090: val_loss did not improve from 1.02670
Epoch 91/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8411 - acc: 0.6897 - val_loss: 1.0309 - val_acc: 0.6241

Epoch 00091: val_loss did not improve from 1.02670
Epoch 92/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8373 - acc: 0.6892 - val_loss: 1.0405 - val_acc: 0.6250

Epoch 00092: val_loss did not improve from 1.02670
Epoch 93/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8458 - acc: 0.6896 - val_loss: 1.0370 - val_acc: 0.6219

Epoch 00093: val_loss did not improve from 1.02670
Epoch 94/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8362 - acc: 0.6909 - val_loss: 1.0317 - val_acc: 0.6269

Epoch 00094: val_loss did not improve from 1.02670
Epoch 95/110
898/897 [==============================] - 47s 53ms/step - loss: 0.8391 - acc: 0.6896 - val_loss: 1.0327 - val_acc: 0.6271

Epoch 00095: val_loss did not improve from 1.02670
Epoch 96/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8379 - acc: 0.6885 - val_loss: 1.0285 - val_acc: 0.6264

Epoch 00096: val_loss did not improve from 1.02670
Epoch 97/110
898/897 [==============================] - 47s 53ms/step - loss: 0.8311 - acc: 0.6912 - val_loss: 1.0294 - val_acc: 0.6280

Epoch 00097: val_loss did not improve from 1.02670
Epoch 98/110
898/897 [==============================] - 50s 55ms/step - loss: 0.8389 - acc: 0.6876 - val_loss: 1.0322 - val_acc: 0.6250

Epoch 00098: val_loss did not improve from 1.02670

Epoch 00098: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 99/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8298 - acc: 0.6944 - val_loss: 1.0295 - val_acc: 0.6247

Epoch 00099: val_loss did not improve from 1.02670
Epoch 100/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8274 - acc: 0.6941 - val_loss: 1.0280 - val_acc: 0.6239

Epoch 00100: val_loss did not improve from 1.02670
Epoch 101/110
898/897 [==============================] - 47s 53ms/step - loss: 0.8240 - acc: 0.6920 - val_loss: 1.0285 - val_acc: 0.6254

Epoch 00101: val_loss did not improve from 1.02670
Epoch 102/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8239 - acc: 0.6962 - val_loss: 1.0300 - val_acc: 0.6241

Epoch 00102: val_loss did not improve from 1.02670
Epoch 103/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8279 - acc: 0.6940 - val_loss: 1.0296 - val_acc: 0.6265

Epoch 00103: val_loss did not improve from 1.02670
Epoch 104/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8275 - acc: 0.6914 - val_loss: 1.0273 - val_acc: 0.6275

Epoch 00104: val_loss did not improve from 1.02670
Epoch 105/110
898/897 [==============================] - 47s 53ms/step - loss: 0.8252 - acc: 0.6927 - val_loss: 1.0297 - val_acc: 0.6268

Epoch 00105: val_loss did not improve from 1.02670
Epoch 106/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8238 - acc: 0.6932 - val_loss: 1.0294 - val_acc: 0.6262

Epoch 00106: val_loss did not improve from 1.02670
Epoch 107/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8271 - acc: 0.6928 - val_loss: 1.0300 - val_acc: 0.6251

Epoch 00107: val_loss did not improve from 1.02670
Epoch 108/110
898/897 [==============================] - 47s 53ms/step - loss: 0.8223 - acc: 0.6964 - val_loss: 1.0298 - val_acc: 0.6251

Epoch 00108: val_loss did not improve from 1.02670
Epoch 109/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8272 - acc: 0.6944 - val_loss: 1.0300 - val_acc: 0.6257

Epoch 00109: val_loss did not improve from 1.02670
Epoch 110/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8276 - acc: 0.6933 - val_loss: 1.0299 - val_acc: 0.6265

Epoch 00110: val_loss did not improve from 1.02670

Epoch 00110: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gainClassification_report</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 539   21   85   31  128   31  156]
 [  22   59    8    3   10    2    5]
 [ 148    6  442   40  162   95  131]
 [  56    2   38 1499   30   44  129]
 [ 130    5  150   58  564   21  288]
 [  31    1   89   44   14  585   36]
 [  77    5   60  105  157   27  809]]
Classification Report
              precision    recall  f1-score   support

       angry       0.54      0.54      0.54       991
     disgust       0.60      0.54      0.57       109
        fear       0.51      0.43      0.47      1024
       happy       0.84      0.83      0.84      1798
         sad       0.53      0.46      0.49      1216
   surprised       0.73      0.73      0.73       800
     neutral       0.52      0.65      0.58      1240

    accuracy                           0.63      7178
   macro avg       0.61      0.60      0.60      7178
weighted avg       0.63      0.63      0.62      7178

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[0]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(array([[ 539,   21,   85,   31,  128,   31,  156],
        [  22,   59,    8,    3,   10,    2,    5],
        [ 148,    6,  442,   40,  162,   95,  131],
        [  56,    2,   38, 1499,   30,   44,  129],
        [ 130,    5,  150,   58,  564,   21,  288],
        [  31,    1,   89,   44,   14,  585,   36],
        [  77,    5,   60,  105,  157,   27,  809]]),
 &#39;              precision    recall  f1-score   support\n\n       angry       0.54      0.54      0.54       991\n     disgust       0.60      0.54      0.57       109\n        fear       0.51      0.43      0.47      1024\n       happy       0.84      0.83      0.84      1798\n         sad       0.53      0.46      0.49      1216\n   surprised       0.73      0.73      0.73       800\n     neutral       0.52      0.65      0.58      1240\n\n    accuracy                           0.63      7178\n   macro avg       0.61      0.60      0.60      7178\nweighted avg       0.63      0.63      0.62      7178\n&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Histogram-Equalization">Histogram Equalization<a class="anchor-link" href="#Histogram-Equalization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">trainCNN</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.

Model: &#34;model_1&#34;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 48, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]         
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]         
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]            
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]         
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]         
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]            
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]         
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]         
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]            
                                                                 batch_normalization_9[0][0]      
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]         
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]         
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]                   
__________________________________________________________________________________________________
add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]            
                                                                 batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]                      
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]                   
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

Epoch 1/110
898/897 [==============================] - 54s 60ms/step - loss: 1.7867 - acc: 0.3227 - val_loss: 2.0384 - val_acc: 0.2335

Epoch 00001: val_loss improved from inf to 2.03839, saving model to _mini_XCEPTION_HE.01-0.23.hdf5
Epoch 2/110
898/897 [==============================] - 47s 52ms/step - loss: 1.5362 - acc: 0.4239 - val_loss: 1.7285 - val_acc: 0.3494

Epoch 00002: val_loss improved from 2.03839 to 1.72848, saving model to _mini_XCEPTION_HE.02-0.35.hdf5
Epoch 3/110
898/897 [==============================] - 46s 52ms/step - loss: 1.4258 - acc: 0.4635 - val_loss: 1.5970 - val_acc: 0.3898

Epoch 00003: val_loss improved from 1.72848 to 1.59698, saving model to _mini_XCEPTION_HE.03-0.39.hdf5
Epoch 4/110
898/897 [==============================] - 47s 53ms/step - loss: 1.3521 - acc: 0.4902 - val_loss: 1.5577 - val_acc: 0.4480

Epoch 00004: val_loss improved from 1.59698 to 1.55767, saving model to _mini_XCEPTION_HE.04-0.45.hdf5
Epoch 5/110
898/897 [==============================] - 47s 52ms/step - loss: 1.3094 - acc: 0.5081 - val_loss: 1.2809 - val_acc: 0.5116

Epoch 00005: val_loss improved from 1.55767 to 1.28092, saving model to _mini_XCEPTION_HE.05-0.51.hdf5
Epoch 6/110
898/897 [==============================] - 47s 53ms/step - loss: 1.2683 - acc: 0.5272 - val_loss: 1.3117 - val_acc: 0.5039

Epoch 00006: val_loss did not improve from 1.28092
Epoch 7/110
898/897 [==============================] - 46s 52ms/step - loss: 1.2450 - acc: 0.5319 - val_loss: 1.4497 - val_acc: 0.4681

Epoch 00007: val_loss did not improve from 1.28092
Epoch 8/110
898/897 [==============================] - 47s 52ms/step - loss: 1.2169 - acc: 0.5446 - val_loss: 1.1894 - val_acc: 0.5391

Epoch 00008: val_loss improved from 1.28092 to 1.18937, saving model to _mini_XCEPTION_HE.08-0.54.hdf5
Epoch 9/110
898/897 [==============================] - 47s 53ms/step - loss: 1.1980 - acc: 0.5525 - val_loss: 1.2630 - val_acc: 0.5316

Epoch 00009: val_loss did not improve from 1.18937
Epoch 10/110
898/897 [==============================] - 47s 53ms/step - loss: 1.1843 - acc: 0.5553 - val_loss: 1.3388 - val_acc: 0.4971

Epoch 00010: val_loss did not improve from 1.18937
Epoch 11/110
898/897 [==============================] - 47s 52ms/step - loss: 1.1734 - acc: 0.5566 - val_loss: 1.1771 - val_acc: 0.5614

Epoch 00011: val_loss improved from 1.18937 to 1.17714, saving model to _mini_XCEPTION_HE.11-0.56.hdf5
Epoch 12/110
898/897 [==============================] - 47s 52ms/step - loss: 1.1573 - acc: 0.5686 - val_loss: 1.1995 - val_acc: 0.5485

Epoch 00012: val_loss did not improve from 1.17714
Epoch 13/110
898/897 [==============================] - 47s 52ms/step - loss: 1.1472 - acc: 0.5730 - val_loss: 1.1997 - val_acc: 0.5503

Epoch 00013: val_loss did not improve from 1.17714
Epoch 14/110
898/897 [==============================] - 47s 52ms/step - loss: 1.1340 - acc: 0.5767 - val_loss: 1.1422 - val_acc: 0.5685

Epoch 00014: val_loss improved from 1.17714 to 1.14225, saving model to _mini_XCEPTION_HE.14-0.57.hdf5
Epoch 15/110
898/897 [==============================] - 47s 52ms/step - loss: 1.1236 - acc: 0.5796 - val_loss: 1.2197 - val_acc: 0.5440

Epoch 00015: val_loss did not improve from 1.14225
Epoch 16/110
898/897 [==============================] - 47s 53ms/step - loss: 1.1139 - acc: 0.5868 - val_loss: 1.1333 - val_acc: 0.5823

Epoch 00016: val_loss improved from 1.14225 to 1.13329, saving model to _mini_XCEPTION_HE.16-0.58.hdf5
Epoch 17/110
898/897 [==============================] - 47s 53ms/step - loss: 1.1065 - acc: 0.5875 - val_loss: 1.1288 - val_acc: 0.5839

Epoch 00017: val_loss improved from 1.13329 to 1.12877, saving model to _mini_XCEPTION_HE.17-0.58.hdf5
Epoch 18/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0965 - acc: 0.5915 - val_loss: 1.1688 - val_acc: 0.5719

Epoch 00018: val_loss did not improve from 1.12877
Epoch 19/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0926 - acc: 0.5914 - val_loss: 1.1274 - val_acc: 0.5766

Epoch 00019: val_loss improved from 1.12877 to 1.12743, saving model to _mini_XCEPTION_HE.19-0.58.hdf5
Epoch 20/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0893 - acc: 0.5922 - val_loss: 1.1470 - val_acc: 0.5628

Epoch 00020: val_loss did not improve from 1.12743
Epoch 21/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0817 - acc: 0.5955 - val_loss: 1.1275 - val_acc: 0.5843

Epoch 00021: val_loss did not improve from 1.12743
Epoch 22/110
898/897 [==============================] - 48s 53ms/step - loss: 1.0771 - acc: 0.5991 - val_loss: 1.1056 - val_acc: 0.5981

Epoch 00022: val_loss improved from 1.12743 to 1.10557, saving model to _mini_XCEPTION_HE.22-0.60.hdf5
Epoch 23/110
898/897 [==============================] - 48s 53ms/step - loss: 1.0752 - acc: 0.5965 - val_loss: 1.0849 - val_acc: 0.5899

Epoch 00023: val_loss improved from 1.10557 to 1.08492, saving model to _mini_XCEPTION_HE.23-0.59.hdf5
Epoch 24/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0665 - acc: 0.6032 - val_loss: 1.1153 - val_acc: 0.5876

Epoch 00024: val_loss did not improve from 1.08492
Epoch 25/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0641 - acc: 0.6026 - val_loss: 1.1499 - val_acc: 0.5711

Epoch 00025: val_loss did not improve from 1.08492
Epoch 26/110
898/897 [==============================] - 47s 53ms/step - loss: 1.0556 - acc: 0.6073 - val_loss: 1.0755 - val_acc: 0.6043

Epoch 00026: val_loss improved from 1.08492 to 1.07554, saving model to _mini_XCEPTION_HE.26-0.60.hdf5
Epoch 27/110
898/897 [==============================] - 47s 53ms/step - loss: 1.0527 - acc: 0.6075 - val_loss: 1.2319 - val_acc: 0.5350

Epoch 00027: val_loss did not improve from 1.07554
Epoch 28/110
898/897 [==============================] - 46s 51ms/step - loss: 1.0458 - acc: 0.6102 - val_loss: 1.0798 - val_acc: 0.6030

Epoch 00028: val_loss did not improve from 1.07554
Epoch 29/110
898/897 [==============================] - 46s 52ms/step - loss: 1.0397 - acc: 0.6112 - val_loss: 1.0745 - val_acc: 0.5931

Epoch 00029: val_loss improved from 1.07554 to 1.07451, saving model to _mini_XCEPTION_HE.29-0.59.hdf5
Epoch 30/110
898/897 [==============================] - 46s 51ms/step - loss: 1.0429 - acc: 0.6092 - val_loss: 1.1367 - val_acc: 0.5812

Epoch 00030: val_loss did not improve from 1.07451
Epoch 31/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0364 - acc: 0.6147 - val_loss: 1.1610 - val_acc: 0.5761

Epoch 00031: val_loss did not improve from 1.07451
Epoch 32/110
898/897 [==============================] - 46s 51ms/step - loss: 1.0395 - acc: 0.6104 - val_loss: 1.0760 - val_acc: 0.5970

Epoch 00032: val_loss did not improve from 1.07451
Epoch 33/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0289 - acc: 0.6179 - val_loss: 1.0686 - val_acc: 0.6045

Epoch 00033: val_loss improved from 1.07451 to 1.06863, saving model to _mini_XCEPTION_HE.33-0.60.hdf5
Epoch 34/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0255 - acc: 0.6172 - val_loss: 1.1659 - val_acc: 0.5659

Epoch 00034: val_loss did not improve from 1.06863
Epoch 35/110
898/897 [==============================] - 46s 52ms/step - loss: 1.0205 - acc: 0.6200 - val_loss: 1.1135 - val_acc: 0.5840

Epoch 00035: val_loss did not improve from 1.06863
Epoch 36/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0156 - acc: 0.6241 - val_loss: 1.0528 - val_acc: 0.6048

Epoch 00036: val_loss improved from 1.06863 to 1.05278, saving model to _mini_XCEPTION_HE.36-0.60.hdf5
Epoch 37/110
898/897 [==============================] - 46s 52ms/step - loss: 1.0137 - acc: 0.6255 - val_loss: 1.0493 - val_acc: 0.6052

Epoch 00037: val_loss improved from 1.05278 to 1.04932, saving model to _mini_XCEPTION_HE.37-0.61.hdf5
Epoch 38/110
898/897 [==============================] - 46s 52ms/step - loss: 1.0111 - acc: 0.6241 - val_loss: 1.1050 - val_acc: 0.5995

Epoch 00038: val_loss did not improve from 1.04932
Epoch 39/110
898/897 [==============================] - 46s 52ms/step - loss: 1.0087 - acc: 0.6251 - val_loss: 1.0701 - val_acc: 0.6050

Epoch 00039: val_loss did not improve from 1.04932
Epoch 40/110
898/897 [==============================] - 47s 52ms/step - loss: 1.0101 - acc: 0.6227 - val_loss: 1.0347 - val_acc: 0.6198

Epoch 00040: val_loss improved from 1.04932 to 1.03471, saving model to _mini_XCEPTION_HE.40-0.62.hdf5
Epoch 41/110
898/897 [==============================] - 47s 53ms/step - loss: 1.0020 - acc: 0.6249 - val_loss: 1.1714 - val_acc: 0.5723

Epoch 00041: val_loss did not improve from 1.03471
Epoch 42/110
898/897 [==============================] - 47s 52ms/step - loss: 0.9979 - acc: 0.6274 - val_loss: 1.0588 - val_acc: 0.6113

Epoch 00042: val_loss did not improve from 1.03471
Epoch 43/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9961 - acc: 0.6288 - val_loss: 1.0722 - val_acc: 0.6109

Epoch 00043: val_loss did not improve from 1.03471
Epoch 44/110
898/897 [==============================] - 45s 51ms/step - loss: 0.9968 - acc: 0.6291 - val_loss: 1.0493 - val_acc: 0.6123

Epoch 00044: val_loss did not improve from 1.03471
Epoch 45/110
898/897 [==============================] - 46s 51ms/step - loss: 0.9947 - acc: 0.6301 - val_loss: 1.0509 - val_acc: 0.6085

Epoch 00045: val_loss did not improve from 1.03471
Epoch 46/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9910 - acc: 0.6327 - val_loss: 1.0309 - val_acc: 0.6258

Epoch 00046: val_loss improved from 1.03471 to 1.03093, saving model to _mini_XCEPTION_HE.46-0.63.hdf5
Epoch 47/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9816 - acc: 0.6366 - val_loss: 1.0427 - val_acc: 0.6112

Epoch 00047: val_loss did not improve from 1.03093
Epoch 48/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9853 - acc: 0.6319 - val_loss: 1.0570 - val_acc: 0.6006

Epoch 00048: val_loss did not improve from 1.03093
Epoch 49/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9823 - acc: 0.6343 - val_loss: 1.0691 - val_acc: 0.5940

Epoch 00049: val_loss did not improve from 1.03093
Epoch 50/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9805 - acc: 0.6336 - val_loss: 1.1051 - val_acc: 0.5878

Epoch 00050: val_loss did not improve from 1.03093
Epoch 51/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9796 - acc: 0.6350 - val_loss: 1.0382 - val_acc: 0.6213

Epoch 00051: val_loss did not improve from 1.03093
Epoch 52/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9817 - acc: 0.6340 - val_loss: 1.0689 - val_acc: 0.6066

Epoch 00052: val_loss did not improve from 1.03093
Epoch 53/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9748 - acc: 0.6366 - val_loss: 1.0878 - val_acc: 0.6089

Epoch 00053: val_loss did not improve from 1.03093
Epoch 54/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9804 - acc: 0.6356 - val_loss: 1.0639 - val_acc: 0.6041

Epoch 00054: val_loss did not improve from 1.03093
Epoch 55/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9668 - acc: 0.6413 - val_loss: 1.0501 - val_acc: 0.6109

Epoch 00055: val_loss did not improve from 1.03093
Epoch 56/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9682 - acc: 0.6401 - val_loss: 1.0483 - val_acc: 0.6106

Epoch 00056: val_loss did not improve from 1.03093
Epoch 57/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9657 - acc: 0.6403 - val_loss: 1.0386 - val_acc: 0.6154

Epoch 00057: val_loss did not improve from 1.03093
Epoch 58/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9663 - acc: 0.6409 - val_loss: 1.1146 - val_acc: 0.5867

Epoch 00058: val_loss did not improve from 1.03093

Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 59/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9198 - acc: 0.6595 - val_loss: 0.9834 - val_acc: 0.6389

Epoch 00059: val_loss improved from 1.03093 to 0.98343, saving model to _mini_XCEPTION_HE.59-0.64.hdf5
Epoch 60/110
898/897 [==============================] - 44s 49ms/step - loss: 0.9109 - acc: 0.6611 - val_loss: 0.9789 - val_acc: 0.6411

Epoch 00060: val_loss improved from 0.98343 to 0.97888, saving model to _mini_XCEPTION_HE.60-0.64.hdf5
Epoch 61/110
898/897 [==============================] - 45s 50ms/step - loss: 0.9005 - acc: 0.6697 - val_loss: 0.9724 - val_acc: 0.6436

Epoch 00061: val_loss improved from 0.97888 to 0.97237, saving model to _mini_XCEPTION_HE.61-0.64.hdf5
Epoch 62/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8971 - acc: 0.6641 - val_loss: 0.9766 - val_acc: 0.6445

Epoch 00062: val_loss did not improve from 0.97237
Epoch 63/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8908 - acc: 0.6687 - val_loss: 0.9860 - val_acc: 0.6395

Epoch 00063: val_loss did not improve from 0.97237
Epoch 64/110
898/897 [==============================] - 45s 50ms/step - loss: 0.8842 - acc: 0.6715 - val_loss: 0.9764 - val_acc: 0.6417

Epoch 00064: val_loss did not improve from 0.97237
Epoch 65/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8843 - acc: 0.6726 - val_loss: 0.9808 - val_acc: 0.6376

Epoch 00065: val_loss did not improve from 0.97237
Epoch 66/110
898/897 [==============================] - 44s 50ms/step - loss: 0.8820 - acc: 0.6725 - val_loss: 0.9739 - val_acc: 0.6436

Epoch 00066: val_loss did not improve from 0.97237
Epoch 67/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8811 - acc: 0.6752 - val_loss: 0.9846 - val_acc: 0.6413

Epoch 00067: val_loss did not improve from 0.97237
Epoch 68/110
898/897 [==============================] - 45s 50ms/step - loss: 0.8761 - acc: 0.6770 - val_loss: 0.9774 - val_acc: 0.6435

Epoch 00068: val_loss did not improve from 0.97237
Epoch 69/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8785 - acc: 0.6742 - val_loss: 0.9778 - val_acc: 0.6447

Epoch 00069: val_loss did not improve from 0.97237
Epoch 70/110
898/897 [==============================] - 45s 51ms/step - loss: 0.8743 - acc: 0.6755 - val_loss: 0.9759 - val_acc: 0.6445

Epoch 00070: val_loss did not improve from 0.97237
Epoch 71/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8697 - acc: 0.6794 - val_loss: 0.9769 - val_acc: 0.6435

Epoch 00071: val_loss did not improve from 0.97237
Epoch 72/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8722 - acc: 0.6760 - val_loss: 0.9821 - val_acc: 0.6408

Epoch 00072: val_loss did not improve from 0.97237
Epoch 73/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8741 - acc: 0.6751 - val_loss: 0.9745 - val_acc: 0.6441

Epoch 00073: val_loss did not improve from 0.97237

Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 74/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8695 - acc: 0.6775 - val_loss: 0.9756 - val_acc: 0.6399

Epoch 00074: val_loss did not improve from 0.97237
Epoch 75/110
898/897 [==============================] - 43s 48ms/step - loss: 0.8686 - acc: 0.6742 - val_loss: 0.9760 - val_acc: 0.6390

Epoch 00075: val_loss did not improve from 0.97237
Epoch 76/110
898/897 [==============================] - 43s 48ms/step - loss: 0.8686 - acc: 0.6800 - val_loss: 0.9752 - val_acc: 0.6389

Epoch 00076: val_loss did not improve from 0.97237
Epoch 77/110
898/897 [==============================] - 45s 50ms/step - loss: 0.8627 - acc: 0.6804 - val_loss: 0.9748 - val_acc: 0.6396

Epoch 00077: val_loss did not improve from 0.97237
Epoch 78/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8605 - acc: 0.6815 - val_loss: 0.9754 - val_acc: 0.6402

Epoch 00078: val_loss did not improve from 0.97237
Epoch 79/110
898/897 [==============================] - 43s 48ms/step - loss: 0.8616 - acc: 0.6807 - val_loss: 0.9762 - val_acc: 0.6389

Epoch 00079: val_loss did not improve from 0.97237
Epoch 80/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8598 - acc: 0.6811 - val_loss: 0.9749 - val_acc: 0.6393

Epoch 00080: val_loss did not improve from 0.97237
Epoch 81/110
898/897 [==============================] - 44s 49ms/step - loss: 0.8616 - acc: 0.6807 - val_loss: 0.9747 - val_acc: 0.6393

Epoch 00081: val_loss did not improve from 0.97237
Epoch 82/110
898/897 [==============================] - 46s 51ms/step - loss: 0.8597 - acc: 0.6786 - val_loss: 0.9756 - val_acc: 0.6396

Epoch 00082: val_loss did not improve from 0.97237
Epoch 83/110
898/897 [==============================] - 46s 51ms/step - loss: 0.8604 - acc: 0.6813 - val_loss: 0.9757 - val_acc: 0.6393

Epoch 00083: val_loss did not improve from 0.97237
Epoch 84/110
898/897 [==============================] - 46s 51ms/step - loss: 0.8594 - acc: 0.6805 - val_loss: 0.9746 - val_acc: 0.6404

Epoch 00084: val_loss did not improve from 0.97237
Epoch 85/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8631 - acc: 0.6809 - val_loss: 0.9755 - val_acc: 0.6404

Epoch 00085: val_loss did not improve from 0.97237

Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Epoch 86/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8591 - acc: 0.6804 - val_loss: 0.9749 - val_acc: 0.6415

Epoch 00086: val_loss did not improve from 0.97237
Epoch 87/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8608 - acc: 0.6794 - val_loss: 0.9748 - val_acc: 0.6413

Epoch 00087: val_loss did not improve from 0.97237
Epoch 88/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8575 - acc: 0.6827 - val_loss: 0.9748 - val_acc: 0.6414

Epoch 00088: val_loss did not improve from 0.97237
Epoch 89/110
898/897 [==============================] - 46s 51ms/step - loss: 0.8585 - acc: 0.6830 - val_loss: 0.9756 - val_acc: 0.6410

Epoch 00089: val_loss did not improve from 0.97237
Epoch 90/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8608 - acc: 0.6812 - val_loss: 0.9748 - val_acc: 0.6396

Epoch 00090: val_loss did not improve from 0.97237
Epoch 91/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8576 - acc: 0.6797 - val_loss: 0.9749 - val_acc: 0.6410

Epoch 00091: val_loss did not improve from 0.97237
Epoch 92/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8633 - acc: 0.6791 - val_loss: 0.9751 - val_acc: 0.6411

Epoch 00092: val_loss did not improve from 0.97237
Epoch 93/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8613 - acc: 0.6777 - val_loss: 0.9753 - val_acc: 0.6404

Epoch 00093: val_loss did not improve from 0.97237
Epoch 94/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8612 - acc: 0.6792 - val_loss: 0.9739 - val_acc: 0.6400

Epoch 00094: val_loss did not improve from 0.97237
Epoch 95/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8622 - acc: 0.6780 - val_loss: 0.9746 - val_acc: 0.6403

Epoch 00095: val_loss did not improve from 0.97237
Epoch 96/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8641 - acc: 0.6750 - val_loss: 0.9751 - val_acc: 0.6404

Epoch 00096: val_loss did not improve from 0.97237
Epoch 97/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8555 - acc: 0.6820 - val_loss: 0.9745 - val_acc: 0.6393

Epoch 00097: val_loss did not improve from 0.97237

Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
Epoch 98/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8659 - acc: 0.6744 - val_loss: 0.9744 - val_acc: 0.6399

Epoch 00098: val_loss did not improve from 0.97237
Epoch 99/110
898/897 [==============================] - 46s 51ms/step - loss: 0.8579 - acc: 0.6800 - val_loss: 0.9754 - val_acc: 0.6411

Epoch 00099: val_loss did not improve from 0.97237
Epoch 100/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8576 - acc: 0.6828 - val_loss: 0.9758 - val_acc: 0.6402

Epoch 00100: val_loss did not improve from 0.97237
Epoch 101/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8650 - acc: 0.6772 - val_loss: 0.9749 - val_acc: 0.6410

Epoch 00101: val_loss did not improve from 0.97237
Epoch 102/110
898/897 [==============================] - 46s 51ms/step - loss: 0.8602 - acc: 0.6775 - val_loss: 0.9751 - val_acc: 0.6396

Epoch 00102: val_loss did not improve from 0.97237
Epoch 103/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8623 - acc: 0.6760 - val_loss: 0.9755 - val_acc: 0.6400

Epoch 00103: val_loss did not improve from 0.97237
Epoch 104/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8589 - acc: 0.6828 - val_loss: 0.9750 - val_acc: 0.6402

Epoch 00104: val_loss did not improve from 0.97237
Epoch 105/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8611 - acc: 0.6788 - val_loss: 0.9743 - val_acc: 0.6402

Epoch 00105: val_loss did not improve from 0.97237
Epoch 106/110
898/897 [==============================] - 48s 53ms/step - loss: 0.8604 - acc: 0.6804 - val_loss: 0.9750 - val_acc: 0.6402

Epoch 00106: val_loss did not improve from 0.97237
Epoch 107/110
898/897 [==============================] - 46s 52ms/step - loss: 0.8571 - acc: 0.6822 - val_loss: 0.9748 - val_acc: 0.6399

Epoch 00107: val_loss did not improve from 0.97237
Epoch 108/110
898/897 [==============================] - 47s 53ms/step - loss: 0.8584 - acc: 0.6828 - val_loss: 0.9746 - val_acc: 0.6400

Epoch 00108: val_loss did not improve from 0.97237
Epoch 109/110
898/897 [==============================] - 47s 52ms/step - loss: 0.8612 - acc: 0.6803 - val_loss: 0.9748 - val_acc: 0.6413

Epoch 00109: val_loss did not improve from 0.97237

Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.
Epoch 110/110
898/897 [==============================] - 47s 53ms/step - loss: 0.8571 - acc: 0.6803 - val_loss: 0.9744 - val_acc: 0.6410

Epoch 00110: val_loss did not improve from 0.97237
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mat</span><span class="p">,</span><span class="n">report</span> <span class="o">=</span> <span class="n">gainClassification_report</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 545    8  101   47  114   34  142]
 [  31   43   12    8   10    1    4]
 [ 130    3  393   47  198  120  133]
 [  32    1   24 1563   39   47   92]
 [ 129    5  142   35  638   17  250]
 [  42    1   68   47   20  594   28]
 [  81    0   63   86  158   27  825]]
Classification Report
              precision    recall  f1-score   support

       angry       0.55      0.55      0.55       991
     disgust       0.70      0.39      0.51       109
        fear       0.49      0.38      0.43      1024
       happy       0.85      0.87      0.86      1798
         sad       0.54      0.52      0.53      1216
   surprised       0.71      0.74      0.72       800
     neutral       0.56      0.67      0.61      1240

    accuracy                           0.64      7178
   macro avg       0.63      0.59      0.60      7178
weighted avg       0.64      0.64      0.64      7178

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Add-Noise">Add Noise<a class="anchor-link" href="#Add-Noise">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">trainCNN</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: &#34;model_2&#34;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 48, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 46, 8)    72          input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 46, 46, 8)    32          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 46, 8)    0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 44, 44, 8)    576         activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 44, 44, 8)    32          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 44, 44, 8)    0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
separable_conv2d_9 (SeparableCo (None, 44, 44, 16)   200         activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_9[0][0]         
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 44, 44, 16)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
separable_conv2d_10 (SeparableC (None, 44, 44, 16)   400         activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_10[0][0]        
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 22, 22, 16)   128         activation_8[0][0]               
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 22, 22, 16)   64          conv2d_10[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_5[0][0]            
                                                                 batch_normalization_17[0][0]     
__________________________________________________________________________________________________
separable_conv2d_11 (SeparableC (None, 22, 22, 32)   656         add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_11[0][0]        
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 22, 22, 32)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
separable_conv2d_12 (SeparableC (None, 22, 22, 32)   1312        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_12[0][0]        
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 11, 11, 32)   512         add_5[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 11, 11, 32)   128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_6[0][0]            
                                                                 batch_normalization_20[0][0]     
__________________________________________________________________________________________________
separable_conv2d_13 (SeparableC (None, 11, 11, 64)   2336        add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_13[0][0]        
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 11, 11, 64)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
separable_conv2d_14 (SeparableC (None, 11, 11, 64)   4672        activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_14[0][0]        
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 6, 6, 64)     2048        add_6[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 6, 6, 64)     256         conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_7[0][0]            
                                                                 batch_normalization_23[0][0]     
__________________________________________________________________________________________________
separable_conv2d_15 (SeparableC (None, 6, 6, 128)    8768        add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_15[0][0]        
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 6, 6, 128)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
separable_conv2d_16 (SeparableC (None, 6, 6, 128)    17536       activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_16[0][0]        
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 3, 3, 128)    8192        add_7[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 3, 3, 128)    512         conv2d_13[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_8[0][0]            
                                                                 batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 3, 3, 7)      8071        add_8[0][0]                      
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 7)            0           conv2d_14[0][0]                  
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

Epoch 1/110
898/897 [==============================] - 311s 346ms/step - loss: 1.8824 - acc: 0.2620 - val_loss: 1.8120 - val_acc: 0.2910

Epoch 00001: val_loss improved from inf to 1.81197, saving model to _mini_XCEPTION_Noise.01-0.29.hdf5
Epoch 2/110
898/897 [==============================] - 307s 341ms/step - loss: 1.7299 - acc: 0.3223 - val_loss: 1.7337 - val_acc: 0.3165

Epoch 00002: val_loss improved from 1.81197 to 1.73373, saving model to _mini_XCEPTION_Noise.02-0.32.hdf5
Epoch 3/110
898/897 [==============================] - 310s 345ms/step - loss: 1.6572 - acc: 0.3486 - val_loss: 1.9773 - val_acc: 0.2497

Epoch 00003: val_loss did not improve from 1.73373
Epoch 4/110
898/897 [==============================] - 311s 346ms/step - loss: 1.6226 - acc: 0.3633 - val_loss: 1.6076 - val_acc: 0.3741

Epoch 00004: val_loss improved from 1.73373 to 1.60761, saving model to _mini_XCEPTION_Noise.04-0.37.hdf5
Epoch 5/110
898/897 [==============================] - 308s 343ms/step - loss: 1.5935 - acc: 0.3812 - val_loss: 1.6019 - val_acc: 0.3819

Epoch 00005: val_loss improved from 1.60761 to 1.60187, saving model to _mini_XCEPTION_Noise.05-0.38.hdf5
Epoch 6/110
898/897 [==============================] - 309s 344ms/step - loss: 1.5685 - acc: 0.3931 - val_loss: 1.6915 - val_acc: 0.3371

Epoch 00006: val_loss did not improve from 1.60187
Epoch 7/110
898/897 [==============================] - 311s 346ms/step - loss: 1.5462 - acc: 0.4037 - val_loss: 1.6028 - val_acc: 0.3642

Epoch 00007: val_loss did not improve from 1.60187
Epoch 8/110
898/897 [==============================] - 308s 343ms/step - loss: 1.5183 - acc: 0.4138 - val_loss: 1.5014 - val_acc: 0.4100

Epoch 00008: val_loss improved from 1.60187 to 1.50138, saving model to _mini_XCEPTION_Noise.08-0.41.hdf5
Epoch 9/110
898/897 [==============================] - 308s 343ms/step - loss: 1.4997 - acc: 0.4218 - val_loss: 1.8064 - val_acc: 0.2937

Epoch 00009: val_loss did not improve from 1.50138
Epoch 10/110
898/897 [==============================] - 310s 345ms/step - loss: 1.4780 - acc: 0.4310 - val_loss: 1.7057 - val_acc: 0.3320

Epoch 00010: val_loss did not improve from 1.50138
Epoch 11/110
898/897 [==============================] - 309s 344ms/step - loss: 1.4624 - acc: 0.4385 - val_loss: 1.4871 - val_acc: 0.4285

Epoch 00011: val_loss improved from 1.50138 to 1.48706, saving model to _mini_XCEPTION_Noise.11-0.43.hdf5
Epoch 12/110
898/897 [==============================] - 307s 342ms/step - loss: 1.4456 - acc: 0.4471 - val_loss: 1.4599 - val_acc: 0.4361

Epoch 00012: val_loss improved from 1.48706 to 1.45992, saving model to _mini_XCEPTION_Noise.12-0.44.hdf5
Epoch 13/110
898/897 [==============================] - 305s 340ms/step - loss: 1.4345 - acc: 0.4512 - val_loss: 1.7707 - val_acc: 0.3259

Epoch 00013: val_loss did not improve from 1.45992
Epoch 14/110
898/897 [==============================] - 310s 346ms/step - loss: 1.4277 - acc: 0.4587 - val_loss: 1.4241 - val_acc: 0.4586

Epoch 00014: val_loss improved from 1.45992 to 1.42414, saving model to _mini_XCEPTION_Noise.14-0.46.hdf5
Epoch 15/110
898/897 [==============================] - 314s 349ms/step - loss: 1.4122 - acc: 0.4626 - val_loss: 1.5861 - val_acc: 0.3853

Epoch 00015: val_loss did not improve from 1.42414
Epoch 16/110
898/897 [==============================] - 308s 343ms/step - loss: 1.4029 - acc: 0.4639 - val_loss: 1.4556 - val_acc: 0.4576

Epoch 00016: val_loss did not improve from 1.42414
Epoch 17/110
898/897 [==============================] - 308s 343ms/step - loss: 1.3955 - acc: 0.4643 - val_loss: 1.9208 - val_acc: 0.3008

Epoch 00017: val_loss did not improve from 1.42414
Epoch 18/110
898/897 [==============================] - 309s 344ms/step - loss: 1.3904 - acc: 0.4690 - val_loss: 1.4143 - val_acc: 0.4733

Epoch 00018: val_loss improved from 1.42414 to 1.41431, saving model to _mini_XCEPTION_Noise.18-0.47.hdf5
Epoch 19/110
898/897 [==============================] - 309s 344ms/step - loss: 1.3784 - acc: 0.4754 - val_loss: 1.5000 - val_acc: 0.4309

Epoch 00019: val_loss did not improve from 1.41431
Epoch 20/110
898/897 [==============================] - 311s 346ms/step - loss: 1.3763 - acc: 0.4778 - val_loss: 1.3614 - val_acc: 0.4844

Epoch 00020: val_loss improved from 1.41431 to 1.36140, saving model to _mini_XCEPTION_Noise.20-0.48.hdf5
Epoch 21/110
898/897 [==============================] - 306s 340ms/step - loss: 1.3662 - acc: 0.4819 - val_loss: 1.4026 - val_acc: 0.4649

Epoch 00021: val_loss did not improve from 1.36140
Epoch 22/110
898/897 [==============================] - 313s 348ms/step - loss: 1.3605 - acc: 0.4805 - val_loss: 1.3597 - val_acc: 0.4745

Epoch 00022: val_loss improved from 1.36140 to 1.35968, saving model to _mini_XCEPTION_Noise.22-0.47.hdf5
Epoch 23/110
898/897 [==============================] - 312s 348ms/step - loss: 1.3553 - acc: 0.4880 - val_loss: 1.4939 - val_acc: 0.4296

Epoch 00023: val_loss did not improve from 1.35968
Epoch 24/110
898/897 [==============================] - 306s 340ms/step - loss: 1.3512 - acc: 0.4868 - val_loss: 1.3621 - val_acc: 0.4901

Epoch 00024: val_loss did not improve from 1.35968
Epoch 25/110
898/897 [==============================] - 303s 338ms/step - loss: 1.3429 - acc: 0.4906 - val_loss: 1.3504 - val_acc: 0.4954

Epoch 00025: val_loss improved from 1.35968 to 1.35036, saving model to _mini_XCEPTION_Noise.25-0.50.hdf5
Epoch 26/110
898/897 [==============================] - 307s 342ms/step - loss: 1.3408 - acc: 0.4903 - val_loss: 1.4189 - val_acc: 0.4567

Epoch 00026: val_loss did not improve from 1.35036
Epoch 27/110
898/897 [==============================] - 306s 341ms/step - loss: 1.3405 - acc: 0.4915 - val_loss: 1.3962 - val_acc: 0.4699

Epoch 00027: val_loss did not improve from 1.35036
Epoch 28/110
898/897 [==============================] - 308s 343ms/step - loss: 1.3274 - acc: 0.4981 - val_loss: 1.4213 - val_acc: 0.4606

Epoch 00028: val_loss did not improve from 1.35036
Epoch 29/110
898/897 [==============================] - 314s 349ms/step - loss: 1.3280 - acc: 0.4962 - val_loss: 1.3744 - val_acc: 0.4801

Epoch 00029: val_loss did not improve from 1.35036
Epoch 30/110
898/897 [==============================] - 314s 350ms/step - loss: 1.3214 - acc: 0.4974 - val_loss: 1.3324 - val_acc: 0.5011

Epoch 00030: val_loss improved from 1.35036 to 1.33237, saving model to _mini_XCEPTION_Noise.30-0.50.hdf5
Epoch 31/110
898/897 [==============================] - 309s 344ms/step - loss: 1.3207 - acc: 0.4995 - val_loss: 1.4380 - val_acc: 0.4691

Epoch 00031: val_loss did not improve from 1.33237
Epoch 32/110
898/897 [==============================] - 310s 346ms/step - loss: 1.3150 - acc: 0.5020 - val_loss: 1.3682 - val_acc: 0.4850

Epoch 00032: val_loss did not improve from 1.33237
Epoch 33/110
898/897 [==============================] - 309s 344ms/step - loss: 1.3122 - acc: 0.5032 - val_loss: 1.4404 - val_acc: 0.4567

Epoch 00033: val_loss did not improve from 1.33237
Epoch 34/110
898/897 [==============================] - 306s 341ms/step - loss: 1.3069 - acc: 0.5053 - val_loss: 1.3081 - val_acc: 0.5128

Epoch 00034: val_loss improved from 1.33237 to 1.30806, saving model to _mini_XCEPTION_Noise.34-0.51.hdf5
Epoch 35/110
898/897 [==============================] - 311s 346ms/step - loss: 1.3043 - acc: 0.5087 - val_loss: 1.4194 - val_acc: 0.4624

Epoch 00035: val_loss did not improve from 1.30806
Epoch 36/110
898/897 [==============================] - 309s 344ms/step - loss: 1.3019 - acc: 0.5045 - val_loss: 1.3774 - val_acc: 0.4855

Epoch 00036: val_loss did not improve from 1.30806
Epoch 37/110
898/897 [==============================] - 308s 343ms/step - loss: 1.3002 - acc: 0.5075 - val_loss: 1.4535 - val_acc: 0.4514

Epoch 00037: val_loss did not improve from 1.30806
Epoch 38/110
898/897 [==============================] - 311s 346ms/step - loss: 1.2950 - acc: 0.5099 - val_loss: 1.2851 - val_acc: 0.5191

Epoch 00038: val_loss improved from 1.30806 to 1.28514, saving model to _mini_XCEPTION_Noise.38-0.52.hdf5
Epoch 39/110
898/897 [==============================] - 313s 348ms/step - loss: 1.2964 - acc: 0.5090 - val_loss: 1.3669 - val_acc: 0.4763

Epoch 00039: val_loss did not improve from 1.28514
Epoch 40/110
898/897 [==============================] - 310s 345ms/step - loss: 1.2930 - acc: 0.5095 - val_loss: 1.3991 - val_acc: 0.4716

Epoch 00040: val_loss did not improve from 1.28514
Epoch 41/110
898/897 [==============================] - 308s 343ms/step - loss: 1.2913 - acc: 0.5117 - val_loss: 1.3690 - val_acc: 0.4831

Epoch 00041: val_loss did not improve from 1.28514
Epoch 42/110
898/897 [==============================] - 311s 346ms/step - loss: 1.2884 - acc: 0.5118 - val_loss: 1.3669 - val_acc: 0.4788

Epoch 00042: val_loss did not improve from 1.28514
Epoch 43/110
898/897 [==============================] - 310s 345ms/step - loss: 1.2819 - acc: 0.5164 - val_loss: 1.4126 - val_acc: 0.4699

Epoch 00043: val_loss did not improve from 1.28514
Epoch 44/110
898/897 [==============================] - 307s 341ms/step - loss: 1.2780 - acc: 0.5166 - val_loss: 1.3456 - val_acc: 0.4969

Epoch 00044: val_loss did not improve from 1.28514
Epoch 45/110
898/897 [==============================] - 313s 349ms/step - loss: 1.2824 - acc: 0.5175 - val_loss: 1.3663 - val_acc: 0.4833

Epoch 00045: val_loss did not improve from 1.28514
Epoch 46/110
898/897 [==============================] - 321s 358ms/step - loss: 1.2719 - acc: 0.5207 - val_loss: 1.3207 - val_acc: 0.5039

Epoch 00046: val_loss did not improve from 1.28514
Epoch 47/110
898/897 [==============================] - 318s 354ms/step - loss: 1.2735 - acc: 0.5175 - val_loss: 1.3127 - val_acc: 0.5201

Epoch 00047: val_loss did not improve from 1.28514
Epoch 48/110
898/897 [==============================] - 305s 339ms/step - loss: 1.2668 - acc: 0.5221 - val_loss: 1.3689 - val_acc: 0.4944

Epoch 00048: val_loss did not improve from 1.28514
Epoch 49/110
898/897 [==============================] - 304s 339ms/step - loss: 1.2672 - acc: 0.5231 - val_loss: 1.3275 - val_acc: 0.5157

Epoch 00049: val_loss did not improve from 1.28514
Epoch 50/110
898/897 [==============================] - 305s 339ms/step - loss: 1.2651 - acc: 0.5240 - val_loss: 1.3673 - val_acc: 0.4889

Epoch 00050: val_loss did not improve from 1.28514

Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 51/110
898/897 [==============================] - 304s 338ms/step - loss: 1.2210 - acc: 0.5400 - val_loss: 1.2466 - val_acc: 0.5357

Epoch 00051: val_loss improved from 1.28514 to 1.24664, saving model to _mini_XCEPTION_Noise.51-0.54.hdf5
Epoch 52/110
898/897 [==============================] - 305s 340ms/step - loss: 1.2013 - acc: 0.5489 - val_loss: 1.2287 - val_acc: 0.5428

Epoch 00052: val_loss improved from 1.24664 to 1.22871, saving model to _mini_XCEPTION_Noise.52-0.54.hdf5
Epoch 53/110
898/897 [==============================] - 307s 341ms/step - loss: 1.1965 - acc: 0.5494 - val_loss: 1.2472 - val_acc: 0.5387

Epoch 00053: val_loss did not improve from 1.22871
Epoch 54/110
898/897 [==============================] - 306s 340ms/step - loss: 1.1978 - acc: 0.5513 - val_loss: 1.2519 - val_acc: 0.5320

Epoch 00054: val_loss did not improve from 1.22871
Epoch 55/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1930 - acc: 0.5488 - val_loss: 1.2227 - val_acc: 0.5390

Epoch 00055: val_loss improved from 1.22871 to 1.22267, saving model to _mini_XCEPTION_Noise.55-0.54.hdf5
Epoch 56/110
898/897 [==============================] - 306s 341ms/step - loss: 1.1876 - acc: 0.5529 - val_loss: 1.2361 - val_acc: 0.5365

Epoch 00056: val_loss did not improve from 1.22267
Epoch 57/110
898/897 [==============================] - 306s 340ms/step - loss: 1.1855 - acc: 0.5531 - val_loss: 1.2171 - val_acc: 0.5436

Epoch 00057: val_loss improved from 1.22267 to 1.21715, saving model to _mini_XCEPTION_Noise.57-0.54.hdf5
Epoch 58/110
898/897 [==============================] - 306s 340ms/step - loss: 1.1838 - acc: 0.5522 - val_loss: 1.2215 - val_acc: 0.5432

Epoch 00058: val_loss did not improve from 1.21715
Epoch 59/110
898/897 [==============================] - 304s 338ms/step - loss: 1.1814 - acc: 0.5594 - val_loss: 1.2355 - val_acc: 0.5375

Epoch 00059: val_loss did not improve from 1.21715
Epoch 60/110
898/897 [==============================] - 305s 339ms/step - loss: 1.1784 - acc: 0.5580 - val_loss: 1.2191 - val_acc: 0.5442

Epoch 00060: val_loss did not improve from 1.21715
Epoch 61/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1797 - acc: 0.5543 - val_loss: 1.2341 - val_acc: 0.5400

Epoch 00061: val_loss did not improve from 1.21715
Epoch 62/110
898/897 [==============================] - 303s 338ms/step - loss: 1.1769 - acc: 0.5575 - val_loss: 1.2571 - val_acc: 0.5301

Epoch 00062: val_loss did not improve from 1.21715
Epoch 63/110
898/897 [==============================] - 304s 338ms/step - loss: 1.1752 - acc: 0.5601 - val_loss: 1.2263 - val_acc: 0.5456

Epoch 00063: val_loss did not improve from 1.21715
Epoch 64/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1699 - acc: 0.5628 - val_loss: 1.2333 - val_acc: 0.5385

Epoch 00064: val_loss did not improve from 1.21715
Epoch 65/110
898/897 [==============================] - 305s 339ms/step - loss: 1.1709 - acc: 0.5582 - val_loss: 1.2454 - val_acc: 0.5300

Epoch 00065: val_loss did not improve from 1.21715
Epoch 66/110
898/897 [==============================] - 306s 340ms/step - loss: 1.1730 - acc: 0.5623 - val_loss: 1.2424 - val_acc: 0.5372

Epoch 00066: val_loss did not improve from 1.21715
Epoch 67/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1659 - acc: 0.5606 - val_loss: 1.2309 - val_acc: 0.5410

Epoch 00067: val_loss did not improve from 1.21715
Epoch 68/110
898/897 [==============================] - 304s 338ms/step - loss: 1.1672 - acc: 0.5600 - val_loss: 1.2299 - val_acc: 0.5424

Epoch 00068: val_loss did not improve from 1.21715
Epoch 69/110
898/897 [==============================] - 305s 339ms/step - loss: 1.1681 - acc: 0.5617 - val_loss: 1.2268 - val_acc: 0.5430

Epoch 00069: val_loss did not improve from 1.21715

Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 70/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1627 - acc: 0.5602 - val_loss: 1.2237 - val_acc: 0.5435

Epoch 00070: val_loss did not improve from 1.21715
Epoch 71/110
898/897 [==============================] - 303s 337ms/step - loss: 1.1590 - acc: 0.5608 - val_loss: 1.2210 - val_acc: 0.5457

Epoch 00071: val_loss did not improve from 1.21715
Epoch 72/110
898/897 [==============================] - 303s 338ms/step - loss: 1.1565 - acc: 0.5671 - val_loss: 1.2223 - val_acc: 0.5446

Epoch 00072: val_loss did not improve from 1.21715
Epoch 73/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1544 - acc: 0.5686 - val_loss: 1.2248 - val_acc: 0.5412

Epoch 00073: val_loss did not improve from 1.21715
Epoch 74/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1567 - acc: 0.5662 - val_loss: 1.2180 - val_acc: 0.5471

Epoch 00074: val_loss did not improve from 1.21715
Epoch 75/110
898/897 [==============================] - 306s 341ms/step - loss: 1.1576 - acc: 0.5679 - val_loss: 1.2216 - val_acc: 0.5451

Epoch 00075: val_loss did not improve from 1.21715
Epoch 76/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1560 - acc: 0.5644 - val_loss: 1.2164 - val_acc: 0.5478

Epoch 00076: val_loss improved from 1.21715 to 1.21644, saving model to _mini_XCEPTION_Noise.76-0.55.hdf5
Epoch 77/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1565 - acc: 0.5608 - val_loss: 1.2166 - val_acc: 0.5450

Epoch 00077: val_loss did not improve from 1.21644
Epoch 78/110
898/897 [==============================] - 306s 340ms/step - loss: 1.1550 - acc: 0.5650 - val_loss: 1.2165 - val_acc: 0.5474

Epoch 00078: val_loss did not improve from 1.21644
Epoch 79/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1527 - acc: 0.5666 - val_loss: 1.2167 - val_acc: 0.5476

Epoch 00079: val_loss did not improve from 1.21644
Epoch 80/110
898/897 [==============================] - 308s 343ms/step - loss: 1.1512 - acc: 0.5666 - val_loss: 1.2172 - val_acc: 0.5483

Epoch 00080: val_loss did not improve from 1.21644
Epoch 81/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1482 - acc: 0.5706 - val_loss: 1.2156 - val_acc: 0.5485

Epoch 00081: val_loss improved from 1.21644 to 1.21560, saving model to _mini_XCEPTION_Noise.81-0.55.hdf5
Epoch 82/110
898/897 [==============================] - 303s 337ms/step - loss: 1.1550 - acc: 0.5634 - val_loss: 1.2152 - val_acc: 0.5475

Epoch 00082: val_loss improved from 1.21560 to 1.21516, saving model to _mini_XCEPTION_Noise.82-0.55.hdf5
Epoch 83/110
898/897 [==============================] - 307s 342ms/step - loss: 1.1575 - acc: 0.5646 - val_loss: 1.2161 - val_acc: 0.5488

Epoch 00083: val_loss did not improve from 1.21516
Epoch 84/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1514 - acc: 0.5665 - val_loss: 1.2215 - val_acc: 0.5446

Epoch 00084: val_loss did not improve from 1.21516
Epoch 85/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1591 - acc: 0.5646 - val_loss: 1.2183 - val_acc: 0.5454

Epoch 00085: val_loss did not improve from 1.21516
Epoch 86/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1495 - acc: 0.5671 - val_loss: 1.2241 - val_acc: 0.5432

Epoch 00086: val_loss did not improve from 1.21516
Epoch 87/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1580 - acc: 0.5649 - val_loss: 1.2217 - val_acc: 0.5446

Epoch 00087: val_loss did not improve from 1.21516
Epoch 88/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1506 - acc: 0.5708 - val_loss: 1.2203 - val_acc: 0.5457

Epoch 00088: val_loss did not improve from 1.21516
Epoch 89/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1511 - acc: 0.5647 - val_loss: 1.2214 - val_acc: 0.5450

Epoch 00089: val_loss did not improve from 1.21516
Epoch 90/110
898/897 [==============================] - 306s 341ms/step - loss: 1.1514 - acc: 0.5662 - val_loss: 1.2236 - val_acc: 0.5429

Epoch 00090: val_loss did not improve from 1.21516
Epoch 91/110
898/897 [==============================] - 303s 337ms/step - loss: 1.1458 - acc: 0.5697 - val_loss: 1.2235 - val_acc: 0.5433

Epoch 00091: val_loss did not improve from 1.21516
Epoch 92/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1586 - acc: 0.5651 - val_loss: 1.2177 - val_acc: 0.5471

Epoch 00092: val_loss did not improve from 1.21516
Epoch 93/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1474 - acc: 0.5677 - val_loss: 1.2226 - val_acc: 0.5433

Epoch 00093: val_loss did not improve from 1.21516
Epoch 94/110
898/897 [==============================] - 306s 341ms/step - loss: 1.1532 - acc: 0.5658 - val_loss: 1.2243 - val_acc: 0.5449

Epoch 00094: val_loss did not improve from 1.21516

Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Epoch 95/110
898/897 [==============================] - 303s 338ms/step - loss: 1.1554 - acc: 0.5645 - val_loss: 1.2246 - val_acc: 0.5443

Epoch 00095: val_loss did not improve from 1.21516
Epoch 96/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1513 - acc: 0.5626 - val_loss: 1.2240 - val_acc: 0.5446

Epoch 00096: val_loss did not improve from 1.21516
Epoch 97/110
898/897 [==============================] - 305s 339ms/step - loss: 1.1494 - acc: 0.5684 - val_loss: 1.2230 - val_acc: 0.5456

Epoch 00097: val_loss did not improve from 1.21516
Epoch 98/110
898/897 [==============================] - 306s 341ms/step - loss: 1.1515 - acc: 0.5660 - val_loss: 1.2231 - val_acc: 0.5450

Epoch 00098: val_loss did not improve from 1.21516
Epoch 99/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1490 - acc: 0.5662 - val_loss: 1.2240 - val_acc: 0.5447

Epoch 00099: val_loss did not improve from 1.21516
Epoch 100/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1533 - acc: 0.5664 - val_loss: 1.2244 - val_acc: 0.5435

Epoch 00100: val_loss did not improve from 1.21516
Epoch 101/110
898/897 [==============================] - 304s 338ms/step - loss: 1.1477 - acc: 0.5679 - val_loss: 1.2216 - val_acc: 0.5463

Epoch 00101: val_loss did not improve from 1.21516
Epoch 102/110
898/897 [==============================] - 302s 336ms/step - loss: 1.1514 - acc: 0.5623 - val_loss: 1.2244 - val_acc: 0.5430

Epoch 00102: val_loss did not improve from 1.21516
Epoch 103/110
898/897 [==============================] - 304s 338ms/step - loss: 1.1533 - acc: 0.5669 - val_loss: 1.2228 - val_acc: 0.5442

Epoch 00103: val_loss did not improve from 1.21516
Epoch 104/110
898/897 [==============================] - 305s 339ms/step - loss: 1.1485 - acc: 0.5681 - val_loss: 1.2231 - val_acc: 0.5442

Epoch 00104: val_loss did not improve from 1.21516
Epoch 105/110
898/897 [==============================] - 304s 338ms/step - loss: 1.1526 - acc: 0.5671 - val_loss: 1.2216 - val_acc: 0.5458

Epoch 00105: val_loss did not improve from 1.21516
Epoch 106/110
898/897 [==============================] - 305s 340ms/step - loss: 1.1492 - acc: 0.5681 - val_loss: 1.2227 - val_acc: 0.5451

Epoch 00106: val_loss did not improve from 1.21516

Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
Epoch 107/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1561 - acc: 0.5657 - val_loss: 1.2236 - val_acc: 0.5453

Epoch 00107: val_loss did not improve from 1.21516
Epoch 108/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1497 - acc: 0.5701 - val_loss: 1.2216 - val_acc: 0.5461

Epoch 00108: val_loss did not improve from 1.21516
Epoch 109/110
898/897 [==============================] - 307s 342ms/step - loss: 1.1480 - acc: 0.5691 - val_loss: 1.2224 - val_acc: 0.5450

Epoch 00109: val_loss did not improve from 1.21516
Epoch 110/110
898/897 [==============================] - 304s 339ms/step - loss: 1.1565 - acc: 0.5656 - val_loss: 1.2208 - val_acc: 0.5454

Epoch 00110: val_loss did not improve from 1.21516
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mat</span><span class="p">,</span><span class="n">report</span> <span class="o">=</span> <span class="n">gainClassification_report</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-32-885dfedb7c4d&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>mat<span class="ansi-blue-fg">,</span>report <span class="ansi-blue-fg">=</span> gainClassification_report<span class="ansi-blue-fg">(</span>model<span class="ansi-blue-fg">,</span>xtest<span class="ansi-blue-fg">,</span>ytest<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;model&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Only-add-Noise-to-the-Training-Image">Only add Noise to the Training Image<a class="anchor-link" href="#Only-add-Noise-to-the-Training-Image">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">save_path</span> <span class="o">=</span> <span class="s1">&#39;/content/drive/My Drive/Colab Notebooks/&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">google.colab</span> <span class="k">import</span> <span class="n">files</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">files</span><span class="o">.</span><span class="n">download</span><span class="p">(</span> <span class="n">save_path</span><span class="o">+</span><span class="s1">&#39;Untitled&#39;</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">trainCNN</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: &#34;model_1&#34;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 48, 48, 1)]  0                                            
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 46, 8)    72          input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 46, 46, 8)    32          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 46, 46, 8)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 44, 44, 8)    576         activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 44, 44, 8)    32          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 44, 44, 8)    0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 44, 44, 16)   200         activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_8[0][0]         
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 44, 44, 16)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
separable_conv2d_9 (SeparableCo (None, 44, 44, 16)   400         activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_9[0][0]         
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 22, 22, 16)   128         activation_7[0][0]               
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 22, 22, 16)   64          conv2d_9[0][0]                   
__________________________________________________________________________________________________
add_4 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_4[0][0]            
                                                                 batch_normalization_16[0][0]     
__________________________________________________________________________________________________
separable_conv2d_10 (SeparableC (None, 22, 22, 32)   656         add_4[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_10[0][0]        
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 22, 22, 32)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
separable_conv2d_11 (SeparableC (None, 22, 22, 32)   1312        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_11[0][0]        
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 11, 11, 32)   512         add_4[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 11, 11, 32)   128         conv2d_10[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_5[0][0]            
                                                                 batch_normalization_19[0][0]     
__________________________________________________________________________________________________
separable_conv2d_12 (SeparableC (None, 11, 11, 64)   2336        add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_12[0][0]        
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 11, 11, 64)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
separable_conv2d_13 (SeparableC (None, 11, 11, 64)   4672        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_13[0][0]        
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 6, 6, 64)     2048        add_5[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 6, 6, 64)     256         conv2d_11[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_6[0][0]            
                                                                 batch_normalization_22[0][0]     
__________________________________________________________________________________________________
separable_conv2d_14 (SeparableC (None, 6, 6, 128)    8768        add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_14[0][0]        
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 6, 6, 128)    0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
separable_conv2d_15 (SeparableC (None, 6, 6, 128)    17536       activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_15[0][0]        
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 3, 3, 128)    8192        add_6[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 3, 3, 128)    512         conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_7[0][0]            
                                                                 batch_normalization_25[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 3, 3, 7)      8071        add_7[0][0]                      
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_13[0][0]                  
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
Epoch 1/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.8403 - accuracy: 0.2928
Epoch 00001: val_loss improved from inf to 1.70210, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.01-0.35.hdf5
898/897 [==============================] - 399s 444ms/step - loss: 1.8403 - accuracy: 0.2926 - val_loss: 1.7021 - val_accuracy: 0.3520
Epoch 2/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.6442 - accuracy: 0.3700
Epoch 00002: val_loss did not improve from 1.70210
898/897 [==============================] - 407s 453ms/step - loss: 1.6443 - accuracy: 0.3699 - val_loss: 1.8422 - val_accuracy: 0.3362
Epoch 3/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.5505 - accuracy: 0.4053
Epoch 00003: val_loss improved from 1.70210 to 1.44628, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.03-0.45.hdf5
898/897 [==============================] - 407s 453ms/step - loss: 1.5506 - accuracy: 0.4052 - val_loss: 1.4463 - val_accuracy: 0.4483
Epoch 4/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.4939 - accuracy: 0.4297
Epoch 00004: val_loss did not improve from 1.44628
898/897 [==============================] - 407s 453ms/step - loss: 1.4937 - accuracy: 0.4299 - val_loss: 1.4469 - val_accuracy: 0.4677
Epoch 5/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.4400 - accuracy: 0.4542
Epoch 00005: val_loss improved from 1.44628 to 1.40981, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.05-0.47.hdf5
898/897 [==============================] - 416s 464ms/step - loss: 1.4402 - accuracy: 0.4542 - val_loss: 1.4098 - val_accuracy: 0.4746
Epoch 6/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.4080 - accuracy: 0.4667
Epoch 00006: val_loss improved from 1.40981 to 1.35061, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.06-0.51.hdf5
898/897 [==============================] - 405s 451ms/step - loss: 1.4080 - accuracy: 0.4667 - val_loss: 1.3506 - val_accuracy: 0.5078
Epoch 7/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.3774 - accuracy: 0.4762
Epoch 00007: val_loss improved from 1.35061 to 1.30461, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.07-0.51.hdf5
898/897 [==============================] - 398s 443ms/step - loss: 1.3774 - accuracy: 0.4761 - val_loss: 1.3046 - val_accuracy: 0.5096
Epoch 8/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.3438 - accuracy: 0.4877
Epoch 00008: val_loss did not improve from 1.30461
898/897 [==============================] - 402s 448ms/step - loss: 1.3439 - accuracy: 0.4877 - val_loss: 1.3193 - val_accuracy: 0.5077
Epoch 9/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.3157 - accuracy: 0.5033
Epoch 00009: val_loss did not improve from 1.30461
898/897 [==============================] - 406s 452ms/step - loss: 1.3158 - accuracy: 0.5033 - val_loss: 1.4073 - val_accuracy: 0.4987
Epoch 10/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.2916 - accuracy: 0.5109
Epoch 00010: val_loss improved from 1.30461 to 1.24856, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.10-0.54.hdf5
898/897 [==============================] - 410s 457ms/step - loss: 1.2915 - accuracy: 0.5108 - val_loss: 1.2486 - val_accuracy: 0.5396
Epoch 11/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.2788 - accuracy: 0.5171
Epoch 00011: val_loss did not improve from 1.24856
898/897 [==============================] - 407s 453ms/step - loss: 1.2789 - accuracy: 0.5171 - val_loss: 1.3119 - val_accuracy: 0.5235
Epoch 12/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.2606 - accuracy: 0.5245
Epoch 00012: val_loss improved from 1.24856 to 1.21675, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.12-0.55.hdf5
898/897 [==============================] - 384s 428ms/step - loss: 1.2606 - accuracy: 0.5244 - val_loss: 1.2168 - val_accuracy: 0.5454
Epoch 13/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.2465 - accuracy: 0.5275
Epoch 00013: val_loss improved from 1.21675 to 1.20176, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.13-0.56.hdf5
898/897 [==============================] - 383s 426ms/step - loss: 1.2466 - accuracy: 0.5273 - val_loss: 1.2018 - val_accuracy: 0.5589
Epoch 14/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.2358 - accuracy: 0.5356
Epoch 00014: val_loss did not improve from 1.20176
898/897 [==============================] - 391s 436ms/step - loss: 1.2355 - accuracy: 0.5357 - val_loss: 1.2695 - val_accuracy: 0.5301
Epoch 15/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.2221 - accuracy: 0.5408
Epoch 00015: val_loss did not improve from 1.20176
898/897 [==============================] - 393s 438ms/step - loss: 1.2220 - accuracy: 0.5409 - val_loss: 1.2025 - val_accuracy: 0.5481
Epoch 16/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.2096 - accuracy: 0.5460
Epoch 00016: val_loss did not improve from 1.20176
898/897 [==============================] - 406s 452ms/step - loss: 1.2096 - accuracy: 0.5461 - val_loss: 1.2148 - val_accuracy: 0.5571
Epoch 17/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1985 - accuracy: 0.5479
Epoch 00017: val_loss did not improve from 1.20176
898/897 [==============================] - 414s 461ms/step - loss: 1.1986 - accuracy: 0.5479 - val_loss: 1.2270 - val_accuracy: 0.5518
Epoch 18/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1886 - accuracy: 0.5536
Epoch 00018: val_loss did not improve from 1.20176
898/897 [==============================] - 420s 467ms/step - loss: 1.1889 - accuracy: 0.5535 - val_loss: 1.2206 - val_accuracy: 0.5591
Epoch 19/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1844 - accuracy: 0.5553
Epoch 00019: val_loss improved from 1.20176 to 1.18302, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.19-0.57.hdf5
898/897 [==============================] - 412s 459ms/step - loss: 1.1847 - accuracy: 0.5552 - val_loss: 1.1830 - val_accuracy: 0.5667
Epoch 20/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1750 - accuracy: 0.5570
Epoch 00020: val_loss improved from 1.18302 to 1.15959, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.20-0.57.hdf5
898/897 [==============================] - 413s 460ms/step - loss: 1.1750 - accuracy: 0.5570 - val_loss: 1.1596 - val_accuracy: 0.5651
Epoch 21/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1672 - accuracy: 0.5595
Epoch 00021: val_loss improved from 1.15959 to 1.15061, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.21-0.57.hdf5
898/897 [==============================] - 415s 462ms/step - loss: 1.1675 - accuracy: 0.5594 - val_loss: 1.1506 - val_accuracy: 0.5705
Epoch 22/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1633 - accuracy: 0.5602
Epoch 00022: val_loss improved from 1.15061 to 1.13801, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.22-0.58.hdf5
898/897 [==============================] - 405s 451ms/step - loss: 1.1631 - accuracy: 0.5602 - val_loss: 1.1380 - val_accuracy: 0.5807
Epoch 23/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1578 - accuracy: 0.5638
Epoch 00023: val_loss improved from 1.13801 to 1.13408, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.23-0.58.hdf5
898/897 [==============================] - 414s 461ms/step - loss: 1.1577 - accuracy: 0.5639 - val_loss: 1.1341 - val_accuracy: 0.5828
Epoch 24/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1496 - accuracy: 0.5685
Epoch 00024: val_loss improved from 1.13408 to 1.10416, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.24-0.59.hdf5
898/897 [==============================] - 413s 460ms/step - loss: 1.1495 - accuracy: 0.5685 - val_loss: 1.1042 - val_accuracy: 0.5911
Epoch 25/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1459 - accuracy: 0.5695
Epoch 00025: val_loss improved from 1.10416 to 1.08970, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.25-0.58.hdf5
898/897 [==============================] - 407s 453ms/step - loss: 1.1462 - accuracy: 0.5694 - val_loss: 1.0897 - val_accuracy: 0.5839
Epoch 26/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1418 - accuracy: 0.5698
Epoch 00026: val_loss did not improve from 1.08970
898/897 [==============================] - 391s 435ms/step - loss: 1.1417 - accuracy: 0.5698 - val_loss: 1.1500 - val_accuracy: 0.5811
Epoch 27/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1344 - accuracy: 0.5704
Epoch 00027: val_loss did not improve from 1.08970
898/897 [==============================] - 383s 426ms/step - loss: 1.1344 - accuracy: 0.5704 - val_loss: 1.1115 - val_accuracy: 0.5911
Epoch 28/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1254 - accuracy: 0.5780
Epoch 00028: val_loss did not improve from 1.08970
898/897 [==============================] - 381s 425ms/step - loss: 1.1257 - accuracy: 0.5779 - val_loss: 1.1647 - val_accuracy: 0.5782
Epoch 29/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1209 - accuracy: 0.5776
Epoch 00029: val_loss did not improve from 1.08970
898/897 [==============================] - 385s 429ms/step - loss: 1.1208 - accuracy: 0.5776 - val_loss: 1.1216 - val_accuracy: 0.5857
Epoch 30/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1259 - accuracy: 0.5783
Epoch 00030: val_loss did not improve from 1.08970
898/897 [==============================] - 382s 425ms/step - loss: 1.1260 - accuracy: 0.5784 - val_loss: 1.1609 - val_accuracy: 0.5789
Epoch 31/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1121 - accuracy: 0.5834
Epoch 00031: val_loss did not improve from 1.08970
898/897 [==============================] - 382s 425ms/step - loss: 1.1121 - accuracy: 0.5832 - val_loss: 1.1188 - val_accuracy: 0.5872
Epoch 32/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1063 - accuracy: 0.5864
Epoch 00032: val_loss did not improve from 1.08970
898/897 [==============================] - 382s 425ms/step - loss: 1.1062 - accuracy: 0.5865 - val_loss: 1.1506 - val_accuracy: 0.5779
Epoch 33/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.1068 - accuracy: 0.5853
Epoch 00033: val_loss did not improve from 1.08970
898/897 [==============================] - 384s 427ms/step - loss: 1.1072 - accuracy: 0.5851 - val_loss: 1.1335 - val_accuracy: 0.5860
Epoch 34/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0985 - accuracy: 0.5881
Epoch 00034: val_loss improved from 1.08970 to 1.08412, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.34-0.60.hdf5
898/897 [==============================] - 382s 426ms/step - loss: 1.0987 - accuracy: 0.5881 - val_loss: 1.0841 - val_accuracy: 0.5964
Epoch 35/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0939 - accuracy: 0.5902
Epoch 00035: val_loss did not improve from 1.08412
898/897 [==============================] - 380s 423ms/step - loss: 1.0940 - accuracy: 0.5902 - val_loss: 1.1607 - val_accuracy: 0.5809
Epoch 36/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0910 - accuracy: 0.5906
Epoch 00036: val_loss did not improve from 1.08412
898/897 [==============================] - 384s 428ms/step - loss: 1.0908 - accuracy: 0.5906 - val_loss: 1.0851 - val_accuracy: 0.6006
Epoch 37/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0892 - accuracy: 0.5934
Epoch 00037: val_loss did not improve from 1.08412
898/897 [==============================] - 399s 445ms/step - loss: 1.0892 - accuracy: 0.5934 - val_loss: 1.1124 - val_accuracy: 0.5875
Epoch 38/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0854 - accuracy: 0.5953
Epoch 00038: val_loss improved from 1.08412 to 1.06924, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.38-0.60.hdf5
898/897 [==============================] - 392s 437ms/step - loss: 1.0853 - accuracy: 0.5952 - val_loss: 1.0692 - val_accuracy: 0.6024
Epoch 39/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0829 - accuracy: 0.5972
Epoch 00039: val_loss did not improve from 1.06924
898/897 [==============================] - 389s 433ms/step - loss: 1.0827 - accuracy: 0.5973 - val_loss: 1.1140 - val_accuracy: 0.5940
Epoch 40/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0757 - accuracy: 0.5986
Epoch 00040: val_loss did not improve from 1.06924
898/897 [==============================] - 382s 425ms/step - loss: 1.0755 - accuracy: 0.5986 - val_loss: 1.0735 - val_accuracy: 0.6048
Epoch 41/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0752 - accuracy: 0.5990
Epoch 00041: val_loss did not improve from 1.06924
898/897 [==============================] - 381s 425ms/step - loss: 1.0750 - accuracy: 0.5991 - val_loss: 1.1340 - val_accuracy: 0.5971
Epoch 42/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0769 - accuracy: 0.6018
Epoch 00042: val_loss improved from 1.06924 to 1.06233, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.42-0.61.hdf5
898/897 [==============================] - 381s 424ms/step - loss: 1.0767 - accuracy: 0.6019 - val_loss: 1.0623 - val_accuracy: 0.6099
Epoch 43/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0667 - accuracy: 0.6018
Epoch 00043: val_loss did not improve from 1.06233
898/897 [==============================] - 381s 425ms/step - loss: 1.0668 - accuracy: 0.6017 - val_loss: 1.1396 - val_accuracy: 0.5921
Epoch 44/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0675 - accuracy: 0.5999
Epoch 00044: val_loss did not improve from 1.06233
898/897 [==============================] - 382s 425ms/step - loss: 1.0674 - accuracy: 0.5998 - val_loss: 1.0635 - val_accuracy: 0.6115
Epoch 45/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0581 - accuracy: 0.6023
Epoch 00045: val_loss did not improve from 1.06233
898/897 [==============================] - 382s 425ms/step - loss: 1.0580 - accuracy: 0.6024 - val_loss: 1.0993 - val_accuracy: 0.5968
Epoch 46/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0623 - accuracy: 0.6039
Epoch 00046: val_loss did not improve from 1.06233
898/897 [==============================] - 393s 438ms/step - loss: 1.0626 - accuracy: 0.6039 - val_loss: 1.1310 - val_accuracy: 0.5961
Epoch 47/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0615 - accuracy: 0.6019
Epoch 00047: val_loss improved from 1.06233 to 1.05568, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.47-0.61.hdf5
898/897 [==============================] - 405s 451ms/step - loss: 1.0616 - accuracy: 0.6019 - val_loss: 1.0557 - val_accuracy: 0.6074
Epoch 48/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0503 - accuracy: 0.6112
Epoch 00048: val_loss did not improve from 1.05568
898/897 [==============================] - 390s 434ms/step - loss: 1.0506 - accuracy: 0.6109 - val_loss: 1.1414 - val_accuracy: 0.5901
Epoch 49/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0551 - accuracy: 0.6069
Epoch 00049: val_loss did not improve from 1.05568
898/897 [==============================] - 383s 427ms/step - loss: 1.0552 - accuracy: 0.6068 - val_loss: 1.0910 - val_accuracy: 0.6057
Epoch 50/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0471 - accuracy: 0.6101
Epoch 00050: val_loss did not improve from 1.05568
898/897 [==============================] - 385s 428ms/step - loss: 1.0473 - accuracy: 0.6101 - val_loss: 1.0884 - val_accuracy: 0.6077
Epoch 51/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0440 - accuracy: 0.6108
Epoch 00051: val_loss did not improve from 1.05568
898/897 [==============================] - 384s 427ms/step - loss: 1.0440 - accuracy: 0.6109 - val_loss: 1.0701 - val_accuracy: 0.6106
Epoch 52/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0430 - accuracy: 0.6091
Epoch 00052: val_loss did not improve from 1.05568
898/897 [==============================] - 386s 429ms/step - loss: 1.0427 - accuracy: 0.6091 - val_loss: 1.0759 - val_accuracy: 0.6081
Epoch 53/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0459 - accuracy: 0.6098
Epoch 00053: val_loss did not improve from 1.05568
898/897 [==============================] - 383s 427ms/step - loss: 1.0459 - accuracy: 0.6098 - val_loss: 1.0741 - val_accuracy: 0.6070
Epoch 54/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0426 - accuracy: 0.6118
Epoch 00054: val_loss did not improve from 1.05568
898/897 [==============================] - 384s 428ms/step - loss: 1.0425 - accuracy: 0.6119 - val_loss: 1.0652 - val_accuracy: 0.6049
Epoch 55/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0379 - accuracy: 0.6145
Epoch 00055: val_loss improved from 1.05568 to 1.04847, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.55-0.61.hdf5
898/897 [==============================] - 384s 428ms/step - loss: 1.0381 - accuracy: 0.6144 - val_loss: 1.0485 - val_accuracy: 0.6148
Epoch 56/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0352 - accuracy: 0.6146
Epoch 00056: val_loss did not improve from 1.04847
898/897 [==============================] - 384s 427ms/step - loss: 1.0350 - accuracy: 0.6148 - val_loss: 1.0814 - val_accuracy: 0.6120
Epoch 57/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0314 - accuracy: 0.6153
Epoch 00057: val_loss did not improve from 1.04847
898/897 [==============================] - 382s 426ms/step - loss: 1.0317 - accuracy: 0.6154 - val_loss: 1.0619 - val_accuracy: 0.6149
Epoch 58/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0327 - accuracy: 0.6171
Epoch 00058: val_loss did not improve from 1.04847
898/897 [==============================] - 389s 433ms/step - loss: 1.0331 - accuracy: 0.6170 - val_loss: 1.0577 - val_accuracy: 0.6117
Epoch 59/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0328 - accuracy: 0.6155
Epoch 00059: val_loss did not improve from 1.04847
898/897 [==============================] - 411s 457ms/step - loss: 1.0329 - accuracy: 0.6154 - val_loss: 1.0600 - val_accuracy: 0.6080
Epoch 60/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0305 - accuracy: 0.6165
Epoch 00060: val_loss did not improve from 1.04847
898/897 [==============================] - 416s 463ms/step - loss: 1.0310 - accuracy: 0.6163 - val_loss: 1.0622 - val_accuracy: 0.6101
Epoch 61/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0251 - accuracy: 0.6164
Epoch 00061: val_loss did not improve from 1.04847
898/897 [==============================] - 417s 464ms/step - loss: 1.0251 - accuracy: 0.6164 - val_loss: 1.0691 - val_accuracy: 0.6080
Epoch 62/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0244 - accuracy: 0.6171
Epoch 00062: val_loss did not improve from 1.04847
898/897 [==============================] - 417s 464ms/step - loss: 1.0241 - accuracy: 0.6173 - val_loss: 1.0831 - val_accuracy: 0.5988
Epoch 63/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0233 - accuracy: 0.6183
Epoch 00063: val_loss improved from 1.04847 to 1.03646, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.63-0.62.hdf5
898/897 [==============================] - 416s 463ms/step - loss: 1.0235 - accuracy: 0.6183 - val_loss: 1.0365 - val_accuracy: 0.6158
Epoch 64/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0198 - accuracy: 0.6175
Epoch 00064: val_loss did not improve from 1.03646
898/897 [==============================] - 418s 465ms/step - loss: 1.0197 - accuracy: 0.6175 - val_loss: 1.1141 - val_accuracy: 0.5928
Epoch 65/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0192 - accuracy: 0.6201
Epoch 00065: val_loss did not improve from 1.03646
898/897 [==============================] - 421s 469ms/step - loss: 1.0191 - accuracy: 0.6202 - val_loss: 1.0747 - val_accuracy: 0.6052
Epoch 66/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0119 - accuracy: 0.6260
Epoch 00066: val_loss did not improve from 1.03646
898/897 [==============================] - 423s 471ms/step - loss: 1.0122 - accuracy: 0.6258 - val_loss: 1.0498 - val_accuracy: 0.6208
Epoch 67/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0143 - accuracy: 0.6235
Epoch 00067: val_loss did not improve from 1.03646
898/897 [==============================] - 425s 473ms/step - loss: 1.0142 - accuracy: 0.6236 - val_loss: 1.0654 - val_accuracy: 0.6142
Epoch 68/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0109 - accuracy: 0.6226
Epoch 00068: val_loss did not improve from 1.03646
898/897 [==============================] - 425s 474ms/step - loss: 1.0110 - accuracy: 0.6227 - val_loss: 1.0842 - val_accuracy: 0.6043
Epoch 69/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0124 - accuracy: 0.6217
Epoch 00069: val_loss did not improve from 1.03646
898/897 [==============================] - 426s 474ms/step - loss: 1.0123 - accuracy: 0.6217 - val_loss: 1.0763 - val_accuracy: 0.6031
Epoch 70/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0090 - accuracy: 0.6242
Epoch 00070: val_loss did not improve from 1.03646
898/897 [==============================] - 426s 475ms/step - loss: 1.0086 - accuracy: 0.6243 - val_loss: 1.0755 - val_accuracy: 0.6155
Epoch 71/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0070 - accuracy: 0.6244
Epoch 00071: val_loss did not improve from 1.03646
898/897 [==============================] - 426s 474ms/step - loss: 1.0071 - accuracy: 0.6244 - val_loss: 1.0685 - val_accuracy: 0.6094
Epoch 72/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0038 - accuracy: 0.6261
Epoch 00072: val_loss did not improve from 1.03646
898/897 [==============================] - 426s 474ms/step - loss: 1.0039 - accuracy: 0.6260 - val_loss: 1.1022 - val_accuracy: 0.5985
Epoch 73/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0084 - accuracy: 0.6215
Epoch 00073: val_loss did not improve from 1.03646
898/897 [==============================] - 426s 474ms/step - loss: 1.0082 - accuracy: 0.6215 - val_loss: 1.0581 - val_accuracy: 0.6170
Epoch 74/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0028 - accuracy: 0.6250
Epoch 00074: val_loss did not improve from 1.03646
898/897 [==============================] - 426s 474ms/step - loss: 1.0030 - accuracy: 0.6249 - val_loss: 1.0570 - val_accuracy: 0.6202
Epoch 75/100
897/897 [============================&gt;.] - ETA: 0s - loss: 1.0008 - accuracy: 0.6268
Epoch 00075: val_loss did not improve from 1.03646

Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
898/897 [==============================] - 426s 475ms/step - loss: 1.0009 - accuracy: 0.6268 - val_loss: 1.1046 - val_accuracy: 0.5921
Epoch 76/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9612 - accuracy: 0.6428
Epoch 00076: val_loss improved from 1.03646 to 1.00991, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.76-0.63.hdf5
898/897 [==============================] - 427s 475ms/step - loss: 0.9615 - accuracy: 0.6428 - val_loss: 1.0099 - val_accuracy: 0.6342
Epoch 77/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9412 - accuracy: 0.6514
Epoch 00077: val_loss improved from 1.00991 to 1.00291, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.77-0.63.hdf5
898/897 [==============================] - 427s 475ms/step - loss: 0.9413 - accuracy: 0.6513 - val_loss: 1.0029 - val_accuracy: 0.6319
Epoch 78/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9363 - accuracy: 0.6552
Epoch 00078: val_loss did not improve from 1.00291
898/897 [==============================] - 425s 473ms/step - loss: 0.9364 - accuracy: 0.6552 - val_loss: 1.0061 - val_accuracy: 0.6361
Epoch 79/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9264 - accuracy: 0.6604
Epoch 00079: val_loss did not improve from 1.00291
898/897 [==============================] - 425s 473ms/step - loss: 0.9266 - accuracy: 0.6603 - val_loss: 1.0070 - val_accuracy: 0.6312
Epoch 80/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9317 - accuracy: 0.6559
Epoch 00080: val_loss did not improve from 1.00291
898/897 [==============================] - 425s 473ms/step - loss: 0.9316 - accuracy: 0.6560 - val_loss: 1.0056 - val_accuracy: 0.6308
Epoch 81/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9268 - accuracy: 0.6568
Epoch 00081: val_loss did not improve from 1.00291
898/897 [==============================] - 423s 471ms/step - loss: 0.9267 - accuracy: 0.6568 - val_loss: 1.0058 - val_accuracy: 0.6310
Epoch 82/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9285 - accuracy: 0.6540
Epoch 00082: val_loss did not improve from 1.00291
898/897 [==============================] - 422s 470ms/step - loss: 0.9284 - accuracy: 0.6540 - val_loss: 1.0036 - val_accuracy: 0.6328
Epoch 83/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9219 - accuracy: 0.6584
Epoch 00083: val_loss improved from 1.00291 to 1.00195, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.83-0.63.hdf5
898/897 [==============================] - 423s 471ms/step - loss: 0.9220 - accuracy: 0.6584 - val_loss: 1.0019 - val_accuracy: 0.6319
Epoch 84/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9233 - accuracy: 0.6552
Epoch 00084: val_loss improved from 1.00195 to 1.00059, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.84-0.63.hdf5
898/897 [==============================] - 421s 469ms/step - loss: 0.9233 - accuracy: 0.6552 - val_loss: 1.0006 - val_accuracy: 0.6314
Epoch 85/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9219 - accuracy: 0.6564
Epoch 00085: val_loss improved from 1.00059 to 0.99984, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.85-0.63.hdf5
898/897 [==============================] - 422s 470ms/step - loss: 0.9218 - accuracy: 0.6565 - val_loss: 0.9998 - val_accuracy: 0.6346
Epoch 86/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9217 - accuracy: 0.6576
Epoch 00086: val_loss improved from 0.99984 to 0.99762, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.86-0.63.hdf5
898/897 [==============================] - 421s 469ms/step - loss: 0.9219 - accuracy: 0.6575 - val_loss: 0.9976 - val_accuracy: 0.6347
Epoch 87/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9175 - accuracy: 0.6590
Epoch 00087: val_loss did not improve from 0.99762
898/897 [==============================] - 420s 468ms/step - loss: 0.9176 - accuracy: 0.6589 - val_loss: 0.9997 - val_accuracy: 0.6329
Epoch 88/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9121 - accuracy: 0.6588
Epoch 00088: val_loss did not improve from 0.99762
898/897 [==============================] - 421s 469ms/step - loss: 0.9121 - accuracy: 0.6588 - val_loss: 1.0004 - val_accuracy: 0.6339
Epoch 89/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9112 - accuracy: 0.6632
Epoch 00089: val_loss did not improve from 0.99762
898/897 [==============================] - 421s 469ms/step - loss: 0.9110 - accuracy: 0.6633 - val_loss: 1.0002 - val_accuracy: 0.6335
Epoch 90/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9150 - accuracy: 0.6611
Epoch 00090: val_loss did not improve from 0.99762
898/897 [==============================] - 421s 468ms/step - loss: 0.9152 - accuracy: 0.6610 - val_loss: 0.9993 - val_accuracy: 0.6314
Epoch 91/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9114 - accuracy: 0.6593
Epoch 00091: val_loss did not improve from 0.99762
898/897 [==============================] - 419s 466ms/step - loss: 0.9113 - accuracy: 0.6594 - val_loss: 1.0035 - val_accuracy: 0.6321
Epoch 92/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9111 - accuracy: 0.6626
Epoch 00092: val_loss improved from 0.99762 to 0.99588, saving model to /content/drive/My Drive/Colab Notebooks/_mini_XCEPTION_Noise_Train.92-0.63.hdf5
898/897 [==============================] - 422s 470ms/step - loss: 0.9112 - accuracy: 0.6625 - val_loss: 0.9959 - val_accuracy: 0.6330
Epoch 93/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9131 - accuracy: 0.6595
Epoch 00093: val_loss did not improve from 0.99588
898/897 [==============================] - 422s 470ms/step - loss: 0.9131 - accuracy: 0.6595 - val_loss: 0.9983 - val_accuracy: 0.6304
Epoch 94/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9057 - accuracy: 0.6651
Epoch 00094: val_loss did not improve from 0.99588
898/897 [==============================] - 419s 467ms/step - loss: 0.9058 - accuracy: 0.6650 - val_loss: 1.0035 - val_accuracy: 0.6315
Epoch 95/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9055 - accuracy: 0.6618
Epoch 00095: val_loss did not improve from 0.99588
898/897 [==============================] - 418s 466ms/step - loss: 0.9054 - accuracy: 0.6619 - val_loss: 1.0009 - val_accuracy: 0.6351
Epoch 96/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9030 - accuracy: 0.6662
Epoch 00096: val_loss did not improve from 0.99588
898/897 [==============================] - 417s 464ms/step - loss: 0.9029 - accuracy: 0.6663 - val_loss: 1.0016 - val_accuracy: 0.6328
Epoch 97/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9109 - accuracy: 0.6645
Epoch 00097: val_loss did not improve from 0.99588
898/897 [==============================] - 416s 464ms/step - loss: 0.9109 - accuracy: 0.6646 - val_loss: 0.9964 - val_accuracy: 0.6356
Epoch 98/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9086 - accuracy: 0.6627
Epoch 00098: val_loss did not improve from 0.99588
898/897 [==============================] - 416s 463ms/step - loss: 0.9086 - accuracy: 0.6626 - val_loss: 1.0027 - val_accuracy: 0.6319
Epoch 99/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9029 - accuracy: 0.6659
Epoch 00099: val_loss did not improve from 0.99588
898/897 [==============================] - 423s 472ms/step - loss: 0.9029 - accuracy: 0.6660 - val_loss: 0.9967 - val_accuracy: 0.6323
Epoch 100/100
897/897 [============================&gt;.] - ETA: 0s - loss: 0.9020 - accuracy: 0.6646
Epoch 00100: val_loss did not improve from 0.99588
898/897 [==============================] - 429s 478ms/step - loss: 0.9018 - accuracy: 0.6648 - val_loss: 1.0007 - val_accuracy: 0.6329
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results-Comparison">Results Comparison<a class="anchor-link" href="#Results-Comparison">&#182;</a></h2><p>Using the same original test set to get the model performance for different image processing Techniques</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Original-Model">Original Model<a class="anchor-link" href="#Original-Model">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">emotion_model_path</span> <span class="o">=</span> <span class="s1">&#39;_mini_XCEPTION.76-0.65.hdf5&#39;</span>
<span class="n">emotion_classifier</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">emotion_model_path</span><span class="p">,</span><span class="nb">compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mat</span><span class="p">,</span><span class="n">report</span> <span class="o">=</span> <span class="n">gainClassification_report</span><span class="p">(</span><span class="n">emotion_classifier</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 619   12   54   32  110   19  145]
 [  14   73    8    0   10    2    2]
 [ 119    6  485   25  186   94  109]
 [  34    0   27 1564   30   39  104]
 [  96    5   89   32  733    7  254]
 [  27    2   63   36   13  640   19]
 [  66    5   36   59  130   14  930]]
Classification Report
              precision    recall  f1-score   support

       angry       0.63      0.62      0.63       991
     disgust       0.71      0.67      0.69       109
        fear       0.64      0.47      0.54      1024
       happy       0.89      0.87      0.88      1798
         sad       0.60      0.60      0.60      1216
   surprised       0.79      0.80      0.79       800
     neutral       0.60      0.75      0.66      1240

    accuracy                           0.70      7178
   macro avg       0.69      0.68      0.69      7178
weighted avg       0.71      0.70      0.70      7178

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="GCN">GCN<a class="anchor-link" href="#GCN">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">emotion_model_path</span> <span class="o">=</span> <span class="s1">&#39;_mini_XCEPTION_GCN.48-0.63.hdf5&#39;</span>
<span class="n">emotion_classifier</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">emotion_model_path</span><span class="p">,</span><span class="nb">compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mat</span><span class="p">,</span><span class="n">report</span> <span class="o">=</span> <span class="n">gainClassification_report</span><span class="p">(</span><span class="n">emotion_classifier</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 581    3   80   42  123   24  138]
 [  36   30   15    4   14    1    9]
 [ 140    2  396   34  193  143  116]
 [  39    0   24 1559   43   35   98]
 [  95    0  118   51  674   18  260]
 [  24    0   73   31   16  639   17]
 [  65    0   51   75  168   25  856]]
Classification Report
              precision    recall  f1-score   support

       angry       0.59      0.59      0.59       991
     disgust       0.86      0.28      0.42       109
        fear       0.52      0.39      0.44      1024
       happy       0.87      0.87      0.87      1798
         sad       0.55      0.55      0.55      1216
   surprised       0.72      0.80      0.76       800
     neutral       0.57      0.69      0.63      1240

    accuracy                           0.66      7178
   macro avg       0.67      0.59      0.61      7178
weighted avg       0.66      0.66      0.65      7178

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="HE">HE<a class="anchor-link" href="#HE">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">emotion_model_path</span> <span class="o">=</span> <span class="s1">&#39;_mini_XCEPTION_HM.84-0.65.hdf5&#39;</span>
<span class="n">emotion_classifier</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">emotion_model_path</span><span class="p">,</span><span class="nb">compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mat</span><span class="p">,</span><span class="n">report</span> <span class="o">=</span> <span class="n">gainClassification_report</span><span class="p">(</span><span class="n">emotion_classifier</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 627    6   65   27  114   20  132]
 [  26   65    9    0    7    0    2]
 [ 123    2  469   29  186  114  101]
 [  25    2   26 1578   38   38   91]
 [  99    3   79   31  748   10  246]
 [  22    2   60   27   16  657   16]
 [  55    0   29   65  146   16  929]]
Classification Report
              precision    recall  f1-score   support

       angry       0.64      0.63      0.64       991
     disgust       0.81      0.60      0.69       109
        fear       0.64      0.46      0.53      1024
       happy       0.90      0.88      0.89      1798
         sad       0.60      0.62      0.61      1216
   surprised       0.77      0.82      0.79       800
     neutral       0.61      0.75      0.67      1240

    accuracy                           0.71      7178
   macro avg       0.71      0.68      0.69      7178
weighted avg       0.71      0.71      0.70      7178

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="GCN+HM">GCN+HM<a class="anchor-link" href="#GCN+HM">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">emotion_model_path</span> <span class="o">=</span> <span class="s1">&#39;_mini_XCEPTION_New.85-0.62.hdf5&#39;</span>
<span class="n">emotion_classifier</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">emotion_model_path</span><span class="p">,</span><span class="nb">compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mat</span><span class="p">,</span><span class="n">report</span> <span class="o">=</span> <span class="n">gainClassification_report</span><span class="p">(</span><span class="n">emotion_classifier</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 505   10   68   55   93   37  223]
 [  29   37    8    8    7    4   16]
 [ 145    6  345   48  120  131  229]
 [  31    0   29 1503   25   50  160]
 [ 104    3  104   72  438   31  464]
 [  24    2   66   45    8  608   47]
 [  44    0   37   68   93   24  974]]
Classification Report
              precision    recall  f1-score   support

       angry       0.57      0.51      0.54       991
     disgust       0.64      0.34      0.44       109
        fear       0.53      0.34      0.41      1024
       happy       0.84      0.84      0.84      1798
         sad       0.56      0.36      0.44      1216
   surprised       0.69      0.76      0.72       800
     neutral       0.46      0.79      0.58      1240

    accuracy                           0.61      7178
   macro avg       0.61      0.56      0.57      7178
weighted avg       0.62      0.61      0.60      7178

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Noise-Added">Noise Added<a class="anchor-link" href="#Noise-Added">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[0]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">emotion_model_path</span> <span class="o">=</span> <span class="s1">&#39;_mini_XCEPTION_Noise_Train.92-0.63.hdf5&#39;</span>
<span class="n">emotion_classifier</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">emotion_model_path</span><span class="p">,</span><span class="nb">compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mat</span><span class="p">,</span><span class="n">report</span> <span class="o">=</span> <span class="n">gainClassification_report</span><span class="p">(</span><span class="n">emotion_classifier</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion Matrix
[[ 588   18   84   41  139   20  101]
 [  17   73    3    2   11    0    3]
 [ 103    6  481   51  174  104  105]
 [  31    0   15 1619   41   33   59]
 [  89    6  122   64  700    7  228]
 [  22    0   57   54   13  628   26]
 [  57    3   55  100  155   20  850]]
Classification Report
              precision    recall  f1-score   support

       angry       0.65      0.59      0.62       991
     disgust       0.69      0.67      0.68       109
        fear       0.59      0.47      0.52      1024
       happy       0.84      0.90      0.87      1798
         sad       0.57      0.58      0.57      1216
   surprised       0.77      0.79      0.78       800
     neutral       0.62      0.69      0.65      1240

    accuracy                           0.69      7178
   macro avg       0.67      0.67      0.67      7178
weighted avg       0.68      0.69      0.68      7178

</pre>
</div>
</div>

</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
